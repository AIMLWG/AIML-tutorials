{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bdd72-3106-4560-8f4c-dfc09c419f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90a17c-eb16-4037-afbb-f5fb6080c0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593622c2-d768-4e9b-8423-97971b827b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66233b-0ef5-4a0c-b3ad-9f9cc76d5c40",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347400d-304a-43a8-bd4b-1367188f8667",
   "metadata": {},
   "source": [
    "[Matthew R. Carbone](https://www.bnl.gov/staff/mcarbone) | _Assistant Computational Scientist, Computational Science Initiative, Brookhaven National Laboratory_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263b8e5-b41c-4370-8d1c-146cd7c376a5",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn what dimensionality reduction means, how you can use it to your advantage in your own work, and what some of the common methods for doing dimensionality reduction are. We will then implement these methods here on some real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fd9ec-564b-4f65-addf-3e6326d47b6c",
   "metadata": {},
   "source": [
    "## What is dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621278f-5c37-4acc-9768-a050c3fbb7f3",
   "metadata": {},
   "source": [
    "[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) is the process of transforming a dataset from a _high dimensional_ to a _low dimensional_ space. This begs the question, **what is dimension?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedfa081-60a3-4161-a578-29c885bedd94",
   "metadata": {},
   "source": [
    "## What is dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858e470-8ce8-4fae-863a-346245ac585d",
   "metadata": {},
   "source": [
    "- Colloquially, when we think of dimension, we think of the three spatial dimensions we live in\n",
    "- More specifically, we are referring to three _degrees of freedom_ in which we can move\n",
    "- A data point is a single example in a data set\n",
    "- Each data point carries with it a number of properties. For example, in a data set of cars, each car will have properties like the numer of wheels, the number of doors, miles per gallon, etc.\n",
    "- The number of properties each data points has is its dimensionality\n",
    "- Note: properties may or may not be independent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672d5dc-1040-4dbb-a2da-32411a072be7",
   "metadata": {},
   "source": [
    "### Check your understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750fe6b-f5ec-40dd-a2ed-d8c98eceade0",
   "metadata": {},
   "source": [
    "Let's look at the Palmer Penguins dataset we played around with in the last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47de4fc-071d-4a36-b39e-70dcc67af2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d499725-8a4b-481d-aa40-2ca31cd09dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "penguins = load_penguins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa02f6-b917-4789-904f-c194e575728c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d2c00-fb9e-4ac2-92fa-205f990c2d5a",
   "metadata": {},
   "source": [
    "What is the dimension of the Palmer Penguins dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9988369-b9d7-4aae-ad25-5119a88772a7",
   "metadata": {},
   "source": [
    "Here's a tricker example. What is the dimensionality of a \"2d color image\"? Hint: it's not 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f8028-33f3-43a8-8bf0-acd491df0c81",
   "metadata": {},
   "source": [
    "# Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73152db-e2d4-47a3-90fe-87816eb63e40",
   "metadata": {},
   "source": [
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) is a statistically rigorous method for reducing the dimensionality of a dataset. It is mostly used as a preliminary analysis and visualization tool. Here are some reference materials you can take a look at in your free time to get a better feel for what this method is and how it works!\n",
    "\n",
    "- [Scikit Learn's decomposition reference](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)\n",
    "- [PCA Explained Visually with Zero Math](https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443e244-414e-43ee-bbc0-0e471ac00b73",
   "metadata": {},
   "source": [
    "## What is PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58029fe-73dc-4bfc-941f-cecff6137080",
   "metadata": {},
   "source": [
    "PCA is arguably the simplest dimensionality reduction method. It ultimately decomposes each data point's $d$-dimensional feature vector into a $d'$-dimensional feature vector, where $d' < d$. The new \"effective features\" lie in a new vector space, which is a linear combination of the old one.\n",
    "\n",
    "To get into some of the details, we have to understand the [covariance Matrix](https://en.wikipedia.org/wiki/Covariance_matrix). The Covariance Matrix $K$ is square, symmetric, positive semi-definite matrix. The diagonals $K_{ii}$ are the variances of each feature. The off-diagonals $K_{ij}$ ($i \\neq j$) are the covarainces between different features. Formally, this means\n",
    "\n",
    "$$K_{ij} = \\mathrm{cov}(X_i, X_j) = \\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_j - \\mathbb{E}[X_j])].$$\n",
    "\n",
    "While in general $X_i$ is a random varaible, in the case of some fixed dataset, $X_i$ is usually a _feature_ of the data. Thus, for a datset of $N$ elements and $d$ features, the covariance matrix is $d \\times d.$\n",
    "\n",
    "PCA can effectively be reduced to **diagonalizing the covariance matrix**. This new diagonal form contains elements which represent the variance of each new axis, and each new axis is an eigenvector of $K.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8667f-c711-498e-a69f-c23d6ecf49c5",
   "metadata": {},
   "source": [
    "## \"Iris\" Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46ea59-c334-4267-b140-197e9a9cf995",
   "metadata": {},
   "source": [
    "The Iris dataset contains three different types of Iris flowers. There are 150 examples and each example has 4 attributes (4 _dimensions_). Let's see if we can use PCA to decompose the dataset from 4 dimensions into just 2! See [here](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) and [here](https://en.wikipedia.org/wiki/Iris_flower_data_set) for more details on the dataset. For now, we'll use a simple processing script to turn the dictionary dataset into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56d128-7a78-4685-9db1-f6b21ea63465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91882f4-317c-4a3f-889b-0430f67f80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "# print(iris[\"DESCR\"])  # For more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c715418-2902-4f64-8094-f70b2c91933e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_iris(iris=iris):\n",
    "    d = {\n",
    "        \"sepal length (cm)\": iris[\"data\"][:, 0],\n",
    "        \"sepal width (cm)\": iris[\"data\"][:, 1],\n",
    "        \"petal length (cm)\": iris[\"data\"][:, 2],\n",
    "        \"petal width (cm)\": iris[\"data\"][:, 3],\n",
    "        \"class\": iris[\"target\"]\n",
    "    }\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77cc17-6bbf-4bd7-a06d-b478385e6875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_data = process_iris(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f17e7-641c-4bee-a508-8917d376e1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40909e8f-5763-42af-b3cb-1ddb80d192f0",
   "metadata": {},
   "source": [
    "Let's do PCA by hand first. Then we can find an easier way of doing it. Below, I've written a function `covariance_matrix`, which assumes an incoming matrix of shape `N` x `d`, where `N` is the number of data points and `d` is the dimensionality of each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841be64-ca6c-454f-b847-482f1f39ffec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def covariance_matrix(X):\n",
    "    \"\"\"Let X be an N x d dataset.\"\"\"\n",
    "    \n",
    "    mu = X.mean(axis=0)  # For each feature, find the mean\n",
    "    X2 = X - mu          # Subtract off the mean from each element\n",
    "    N = X.shape[0]       # Total number of data points\n",
    "    return (X2.T @ X2) / (N - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef374e-a426-488f-b785-e7b074a8724d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = iris_data.iloc[:, :4].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fddd24f-4e42-4e55-99a0-bb493c7449bd",
   "metadata": {},
   "source": [
    "We'll also want to scale our data to 0 mean and unit variance before we go forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc82df-ede2-44a1-854b-9fed9e5084b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0, keepdims=True)) / (1e-8 + X.std(axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a20e15-8c90-44e6-98c1-5cec1573210c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = covariance_matrix(X_scaled)\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1415d-5aaf-46a9-bbec-bfa4c46f90f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numpy requires a different convention, hence X.T\n",
    "np.allclose(K, np.cov(X_scaled.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95144290-9c93-4d81-9e44-daab3161558b",
   "metadata": {},
   "source": [
    "Now that we have the covariance matrix, it's time to diagonalize it. What does this mean? Essentially, it means we are looking for a transformation:\n",
    "\n",
    "$$V^T K V = K'$$\n",
    "\n",
    "such that $K'$ is diagonal. We're not going to go into the details here, but the way to do this is via the eigenvector decomposition of the matrix. Luckily, we have packages for this. `np.linalg.eig` provies a convenience method for calculating the eigenvalues `w` and eigenvectors `V` (in matrix form) of any provided square matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d482d71-76f3-4833-b12c-8916e7f150f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w, V = np.linalg.eig(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a6e0a-edb4-4f23-ba87-90a56f6ea1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K_prime = V.T @ K @ V         # Test the above transformation!\n",
    "K_prime[K_prime < 1e-14] = 0  # Convenience for visualization only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1360f-f648-48b2-8931-d5ee8de873a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845749e2-e684-4b0b-92d3-891812133eec",
   "metadata": {},
   "source": [
    "Note that the eigenvalues `w` appear in the diagonals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a747a3-8e5c-42a1-8165-635ea875100a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1de6d-bdcf-41c3-b223-cea5244a577d",
   "metadata": {},
   "source": [
    "and that these eigenvectors are orthogonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb233c-473c-43e6-9346-024125bef821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ii, jj in itertools.combinations(range(4), 2):\n",
    "    assert np.abs(np.dot(V[:, ii], V[:, jj])) < 1e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72165df3-c910-401a-aaef-e8f966b529f1",
   "metadata": {},
   "source": [
    "So what now? Well, we have the eigenvectors of the covariance matrix, but what do we do with them? It turns out that these eigenvectors actually represent the linear combination of the features of the original dataset such that the _first_ \"direction\" captures the most possible variance, the _second_ \"direction\" captures the second most possible variance, etc. We can execute this transformation via yet another simple matrix operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242aa14a-6ef6-4fe4-8db6-84b651f0b258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dae3b6-a7b4-4856-ba0b-69169c47b5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf19b5f-3da7-4a05-abae-fc8121f48cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "principal_values = X_scaled @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8efd2d-3a96-4d75-a87c-fccda564d51d",
   "metadata": {},
   "source": [
    "Don't want to do all of this work? Don't worry! `scikit-learn` has you covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee03d30-6049-44d0-8368-ee0454b9b3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c6e5e-17c6-4743-991c-3b18ec0da20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(4)\n",
    "principal_values_via_sklearn = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b868d8f-bb78-4632-bfa2-69fcbf686678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eigenvectors are the same up to a +/- sign, which is fine\n",
    "np.allclose(np.abs(principal_values), np.abs(principal_values_via_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c24b7-1ba8-4cb5-8ef4-da7b83e77e73",
   "metadata": {},
   "source": [
    "The key improvement that `scikit-learn` will offer is that you don't actually have to compute all eigenvectors, as doing so is expensive. There are cheaper ways to do so, and you could do something like `pca = PCA(2)`, which tells `scikit-learn` to only compute the first to principal components. This can be very useful when you're dealing with large datasets, and performing PCA is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf1cbc-63e8-4d0b-90ca-2e111f121a9c",
   "metadata": {},
   "source": [
    "## \"Labeled Faces in the Wild\" Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21bfa2-dc85-4826-9411-eb2bd2d33ef1",
   "metadata": {},
   "source": [
    "# Real research example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bd715-4fc9-4700-9636-f76c9c9e4265",
   "metadata": {},
   "source": [
    "We now follow along with the manuscript Torrisi _et al_ to demonstrate how a random forest was used in a somewhat recent, highly cited research work. The data we pull below is available [open access](https://data.matr.io/4/).\n",
    "\n",
    "S. B. Torrisi, M. R. Carbone, B. A. Rohr, J. H. Montoya, Y. Ha, J. Yano, S. K. Suram & L. Hung. [Random forest machine learning models for interpretable X-ray absorption near-edge structure spectrum-property relationships.](https://www.nature.com/articles/s41524-020-00376-6) npj Comput. Mater. 6, 109 (2020).\n",
    "\n",
    "First, we have to get the data. To do this, we use the `requests` module to directly pull the content of the webpage, and then parse that specific format (which despite the extension is not exactly JSON). It is not important to understand the particulars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e874571-bafe-42da-8da0-cf4810f57ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595573ec-f6c4-4e7f-ad28-792e3d1b073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s3.amazonaws.com/publications.matr.io/4/deployment/data/files/spectral_data/Ti_XY.json\"\n",
    "r = requests.get(url)\n",
    "text = r.text.split(\"\\n\")\n",
    "data = [json.loads(xx) for xx in text[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea4d06-f3bb-46e8-8217-b3b44fc18cbd",
   "metadata": {},
   "source": [
    "Get the inputs and outputs from this list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161e1c6-f3ae-4433-a98b-1f4cb73134c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_grid = data[0][\"E\"]\n",
    "spectra = np.array([\n",
    "    dat[\"mu\"] for dat in data\n",
    "    if dat[\"one_hot_coord\"] is not None\n",
    "])\n",
    "coordinations = np.array([\n",
    "    dat[\"coordination\"] for dat in data\n",
    "    if dat[\"one_hot_coord\"] is not None\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b07d9-3e9c-4831-bbfb-9d167d7544f9",
   "metadata": {},
   "source": [
    "Like the Palmer Penguins dataset, we have reduced our classification task to a 3-class classification problem. In this case, it is the coordination number of an X-ray-absorbing atom! If you don't know what this means, don't worry too much about it. We're simply trying to classify whether or not an X-ray absorption spectrum can be used to predict this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3ab7a-949d-4fe0-aa7b-32e9f2bd9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(coordinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce811530-c641-4f13-b0d5-6ab20137d954",
   "metadata": {},
   "source": [
    "Here are what some of the spectra look like. These are our input features, while the classes above are our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399bbda-a374-4be1-b4c8-a98e14bf5f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2, 1))\n",
    "\n",
    "for spec in spectra[::50]:\n",
    "    ax.plot(e_grid, spec, color=\"black\", alpha=0.1)\n",
    "\n",
    "ax.set_ylabel(\"$\\mu(E)$ / a.u.\")\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"$E$ / e.V.\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
