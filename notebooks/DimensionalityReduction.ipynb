{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bdd72-3106-4560-8f4c-dfc09c419f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90a17c-eb16-4037-afbb-f5fb6080c0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3ca79-c3c5-47b2-b9f0-249d3e2a7ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593622c2-d768-4e9b-8423-97971b827b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66233b-0ef5-4a0c-b3ad-9f9cc76d5c40",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347400d-304a-43a8-bd4b-1367188f8667",
   "metadata": {},
   "source": [
    "[Matthew R. Carbone](https://www.bnl.gov/staff/mcarbone) | _Assistant Computational Scientist, Computational Science Initiative, Brookhaven National Laboratory_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263b8e5-b41c-4370-8d1c-146cd7c376a5",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn what dimensionality reduction means, how you can use it to your advantage in your own work, and what some of the common methods for doing dimensionality reduction are. We will then implement these methods here on some real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fd9ec-564b-4f65-addf-3e6326d47b6c",
   "metadata": {},
   "source": [
    "## What is dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621278f-5c37-4acc-9768-a050c3fbb7f3",
   "metadata": {},
   "source": [
    "[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) is the process of transforming a dataset from a _high dimensional_ to a _low dimensional_ space. This begs the question, **what is dimension?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedfa081-60a3-4161-a578-29c885bedd94",
   "metadata": {},
   "source": [
    "## What is dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858e470-8ce8-4fae-863a-346245ac585d",
   "metadata": {},
   "source": [
    "- Colloquially, when we think of dimension, we think of the three spatial dimensions we live in\n",
    "- More specifically, we are referring to three _degrees of freedom_ in which we can move\n",
    "- A data point is a single example in a data set\n",
    "- Each data point carries with it a number of properties. For example, in a data set of cars, each car will have properties like the numer of wheels, the number of doors, miles per gallon, etc.\n",
    "- The number of properties each data points has is its dimensionality\n",
    "- Note: properties may or may not be independent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672d5dc-1040-4dbb-a2da-32411a072be7",
   "metadata": {},
   "source": [
    "### Check your understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750fe6b-f5ec-40dd-a2ed-d8c98eceade0",
   "metadata": {},
   "source": [
    "Let's look at the Palmer Penguins dataset we played around with in the last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47de4fc-071d-4a36-b39e-70dcc67af2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d499725-8a4b-481d-aa40-2ca31cd09dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "penguins = load_penguins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa02f6-b917-4789-904f-c194e575728c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d2c00-fb9e-4ac2-92fa-205f990c2d5a",
   "metadata": {},
   "source": [
    "What is the dimension of the Palmer Penguins dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9988369-b9d7-4aae-ad25-5119a88772a7",
   "metadata": {},
   "source": [
    "Here's a tricker example. What is the dimensionality of a \"2d color image\"? Hint: it's not 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f8028-33f3-43a8-8bf0-acd491df0c81",
   "metadata": {},
   "source": [
    "# Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73152db-e2d4-47a3-90fe-87816eb63e40",
   "metadata": {},
   "source": [
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) is a statistically rigorous method for reducing the dimensionality of a dataset. It is mostly used as a preliminary analysis and visualization tool. Here are some reference materials you can take a look at in your free time to get a better feel for what this method is and how it works!\n",
    "\n",
    "- [Scikit Learn's decomposition reference](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)\n",
    "- [PCA Explained Visually with Zero Math](https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443e244-414e-43ee-bbc0-0e471ac00b73",
   "metadata": {},
   "source": [
    "## What is PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58029fe-73dc-4bfc-941f-cecff6137080",
   "metadata": {},
   "source": [
    "PCA is arguably the simplest dimensionality reduction method. It ultimately decomposes each data point's $d$-dimensional feature vector into a $d'$-dimensional feature vector, where $d' < d$. The new \"effective features\" lie in a new vector space, which is a linear combination of the old one.\n",
    "\n",
    "To get into some of the details, we have to understand the [covariance Matrix](https://en.wikipedia.org/wiki/Covariance_matrix). The Covariance Matrix $K$ is square, symmetric, positive semi-definite matrix. The diagonals $K_{ii}$ are the variances of each feature. The off-diagonals $K_{ij}$ ($i \\neq j$) are the covarainces between different features. Formally, this means\n",
    "\n",
    "$$K_{ij} = \\mathrm{cov}(X_i, X_j) = \\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_j - \\mathbb{E}[X_j])].$$\n",
    "\n",
    "While in general $X_i$ is a random varaible, in the case of some fixed dataset, $X_i$ is usually a _feature_ of the data. Thus, for a datset of $N$ elements and $d$ features, the covariance matrix is $d \\times d.$\n",
    "\n",
    "PCA can effectively be reduced to **diagonalizing the covariance matrix**. This new diagonal form contains elements which represent the variance of each new axis, and each new axis is an eigenvector of $K.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8667f-c711-498e-a69f-c23d6ecf49c5",
   "metadata": {},
   "source": [
    "## \"Iris\" Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46ea59-c334-4267-b140-197e9a9cf995",
   "metadata": {},
   "source": [
    "The Iris dataset contains three different types of Iris flowers. There are 150 examples and each example has 4 attributes (4 _dimensions_). Let's see if we can use PCA to decompose the dataset from 4 dimensions into just 2! See [here](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) and [here](https://en.wikipedia.org/wiki/Iris_flower_data_set) for more details on the dataset. For now, we'll use a simple processing script to turn the dictionary dataset into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56d128-7a78-4685-9db1-f6b21ea63465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91882f4-317c-4a3f-889b-0430f67f80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "# print(iris[\"DESCR\"])  # For more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c715418-2902-4f64-8094-f70b2c91933e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_iris(iris=iris):\n",
    "    d = {\n",
    "        \"sepal length (cm)\": iris[\"data\"][:, 0],\n",
    "        \"sepal width (cm)\": iris[\"data\"][:, 1],\n",
    "        \"petal length (cm)\": iris[\"data\"][:, 2],\n",
    "        \"petal width (cm)\": iris[\"data\"][:, 3],\n",
    "        \"class\": iris[\"target\"]\n",
    "    }\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77cc17-6bbf-4bd7-a06d-b478385e6875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_data = process_iris(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f17e7-641c-4bee-a508-8917d376e1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40909e8f-5763-42af-b3cb-1ddb80d192f0",
   "metadata": {},
   "source": [
    "Let's do PCA by hand first. Then we can find an easier way of doing it. Below, I've written a function `covariance_matrix`, which assumes an incoming matrix of shape `N` x `d`, where `N` is the number of data points and `d` is the dimensionality of each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841be64-ca6c-454f-b847-482f1f39ffec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def covariance_matrix(X):\n",
    "    \"\"\"Let X be an N x d dataset.\"\"\"\n",
    "    \n",
    "    mu = X.mean(axis=0)  # For each feature, find the mean\n",
    "    X2 = X - mu          # Subtract off the mean from each element\n",
    "    N = X.shape[0]       # Total number of data points\n",
    "    return (X2.T @ X2) / (N - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef374e-a426-488f-b785-e7b074a8724d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = iris_data.iloc[:, :4].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a1f44-09de-45a8-954a-ca574ba34225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y = iris_data[\"class\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fddd24f-4e42-4e55-99a0-bb493c7449bd",
   "metadata": {},
   "source": [
    "We'll also want to scale our data to 0 mean and unit variance before we go forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc82df-ede2-44a1-854b-9fed9e5084b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0, keepdims=True)) / (1e-8 + X.std(axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a20e15-8c90-44e6-98c1-5cec1573210c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = covariance_matrix(X_scaled)\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1415d-5aaf-46a9-bbec-bfa4c46f90f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numpy requires a different convention, hence X.T\n",
    "np.allclose(K, np.cov(X_scaled.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95144290-9c93-4d81-9e44-daab3161558b",
   "metadata": {},
   "source": [
    "Now that we have the covariance matrix, it's time to diagonalize it. What does this mean? Essentially, it means we are looking for a transformation:\n",
    "\n",
    "$$V^T K V = K'$$\n",
    "\n",
    "such that $K'$ is diagonal. We're not going to go into the details here, but the way to do this is via the eigenvector decomposition of the matrix. Luckily, we have packages for this. `np.linalg.eig` provies a convenience method for calculating the eigenvalues `w` and eigenvectors `V` (in matrix form) of any provided square matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d482d71-76f3-4833-b12c-8916e7f150f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w, V = np.linalg.eig(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a6e0a-edb4-4f23-ba87-90a56f6ea1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K_prime = V.T @ K @ V         # Test the above transformation!\n",
    "K_prime[K_prime < 1e-14] = 0  # Convenience for visualization only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1360f-f648-48b2-8931-d5ee8de873a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845749e2-e684-4b0b-92d3-891812133eec",
   "metadata": {},
   "source": [
    "Note that the eigenvalues `w` appear in the diagonals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a747a3-8e5c-42a1-8165-635ea875100a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1de6d-bdcf-41c3-b223-cea5244a577d",
   "metadata": {},
   "source": [
    "and that these eigenvectors are orthogonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb233c-473c-43e6-9346-024125bef821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ii, jj in itertools.combinations(range(4), 2):\n",
    "    assert np.abs(np.dot(V[:, ii], V[:, jj])) < 1e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72165df3-c910-401a-aaef-e8f966b529f1",
   "metadata": {},
   "source": [
    "So what now? Well, we have the eigenvectors of the covariance matrix, but what do we do with them? It turns out that these eigenvectors actually represent the linear combination of the features of the original dataset such that the _first_ \"direction\" captures the most possible variance, the _second_ \"direction\" captures the second most possible variance, etc. We can execute this transformation via yet another simple matrix operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579e8d9-9153-4854-a1a0-c98b9840e68e",
   "metadata": {},
   "source": [
    "Another way to think about this procedure is that it takes a linear combination of features such that those features have zero correlation ([though this does not necessarily imply independence](https://towardsdatascience.com/independence-covariance-and-correlation-between-two-random-variables-197022116f93)). These \"directions\" corresponding to linear combinations are \"ranked\" in order of the amount of variance they capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242aa14a-6ef6-4fe4-8db6-84b651f0b258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dae3b6-a7b4-4856-ba0b-69169c47b5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf19b5f-3da7-4a05-abae-fc8121f48cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "principal_values = X_scaled @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8efd2d-3a96-4d75-a87c-fccda564d51d",
   "metadata": {},
   "source": [
    "Don't want to do all of this work? Don't worry! `scikit-learn` has you covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee03d30-6049-44d0-8368-ee0454b9b3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c6e5e-17c6-4743-991c-3b18ec0da20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(4)\n",
    "principal_values_via_sklearn = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b868d8f-bb78-4632-bfa2-69fcbf686678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eigenvectors are the same up to a +/- sign, which is fine\n",
    "np.allclose(np.abs(principal_values), np.abs(principal_values_via_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c24b7-1ba8-4cb5-8ef4-da7b83e77e73",
   "metadata": {},
   "source": [
    "The key improvement that `scikit-learn` will offer is that you don't actually have to compute all eigenvectors, as doing so is expensive. There are cheaper ways to do so, and you could do something like `pca = PCA(2)`, which tells `scikit-learn` to only compute the first to principal components. This can be very useful when you're dealing with large datasets, and performing PCA is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770ef78-1a18-4e5f-acd2-58ae5ca9f22f",
   "metadata": {},
   "source": [
    "### Inspect the Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c99e3c-74ac-470f-901d-5759a08534f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "for ii in range(4):\n",
    "    ax.plot(pca.components_[:, ii])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee24775-026e-44c8-9bdc-98f09bf785d6",
   "metadata": {},
   "source": [
    "### Make the \"PCA plot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f003c-5e38-46c4-9fc6-1264cc4cc8f0",
   "metadata": {},
   "source": [
    "The primary utility of PCA comes from its ability to decompose very high dimensional data into only a few dimensions efficiently. Usually, we choose two dimensions so we can make a scatter plot of the new, reduced-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3692218-288d-42e7-b36f-d4a5bc5debe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "principal_values_via_sklearn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e09da-5d72-4a36-b149-468be997d995",
   "metadata": {},
   "source": [
    "Usually, we label the point by the classes. Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642ed14-f5d7-45f9-b10b-04c053aaa281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "\n",
    "ax.scatter(principal_values_via_sklearn[:, 0], principal_values_via_sklearn[:, 1], c=Y)\n",
    "\n",
    "ax.set_xlabel(\"$z_1$\")\n",
    "ax.set_ylabel(\"$z_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8545e-893b-46d0-a356-bfc01dbcb0e5",
   "metadata": {},
   "source": [
    "Clearly, we can see that the data \"clusters\" in a reasonable way. So what does this mean? It's a good indicator that a predictive algorithm will be performant. It also can give us an idea as to which labels in an arbitrary dataset \"correlate\" well will the features. In this case, there's only one label, but in general, there may be many labels you wish to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf1cbc-63e8-4d0b-90ca-2e111f121a9c",
   "metadata": {},
   "source": [
    "## \"Labeled Faces in the Wild\" Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339bbd5f-ab7c-4b3e-92a2-5e216790fe78",
   "metadata": {},
   "source": [
    "Let's take a look at another example which can hopefully give you more of an idea as to how PCA works. We'll be using the Labeled Faces in the Wild (LFW) dataset, and following along with a sklearn tutorial [here](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092ce1b-c75f-49ae-9a55-1ba8483afe37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc20c0f-838b-4c9f-ae5a-fd1fc9175630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931f877-6c44-4ee6-b712-ff40e2bb4ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Input shapes\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928936a-38a8-4a0c-adcc-5d9a0df4ca96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(1, 1.5))\n",
    "\n",
    "ii = 6\n",
    "ax.imshow(lfw_people.images[ii], cmap=plt.cm.gray)\n",
    "ax.set_title(target_names[y[ii]], fontsize=8)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.spines[['right', 'top', \"left\", \"bottom\"]].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d14e9-1859-4ad4-ba12-8a1c84919b1c",
   "metadata": {},
   "source": [
    "The dimensionality of each image is `50 x 37` pixels, for a whopping 1850 degrees of freedom. Note that although there are many degrees of freedom, these are images, which means that pixels are almost always locally correlated. Therefore, the effective dimensionality of the images are almost certainly much lower than this. While convolutional neural networks are the algorithm of choice for supervised problems dealing with images, we're going to simply perform PCA on the image data, and use reduced dimensional data to make predictions using a simpler algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d6e9c-4c5c-45ae-a4fd-c40cb06fd226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = lfw_people.images.shape[0]\n",
    "image_shape = lfw_people.images[0].shape  # We'll need the image shape for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd950489-05cc-4662-9ebb-81803be91cfe",
   "metadata": {},
   "source": [
    "First, we flatten and scale the data. At this point, the data is no longer an \"image\", it is just a long feature vector, where each index is a pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9bd87-ed81-40d6-af71-da38c85598fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = lfw_people.images.reshape(N, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dc5eb-b96a-439e-8d14-47fdfd8485f8",
   "metadata": {},
   "source": [
    "We'll do a train/test split and then scale the data. Question: why do we \"fit\" the scaler on only the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d13a4-63a7-4e00-abc0-9a770001b717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2a50a-c42f-433e-b25e-7e33a3e35355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffcbea-eb27-42b5-a5e2-9673615ac95d",
   "metadata": {},
   "source": [
    "Now we apply PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fab9ea-6214-455f-af70-c3ce0e3809f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_components = 150\n",
    "pca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8ff72-d955-4da6-81b1-2bd9028b2b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigenfaces = pca.components_.reshape(n_components, *image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2d739-8dee-4906-8579-cf3c8293412b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(10, 1))\n",
    "\n",
    "for ii, face in enumerate(eigenfaces[:10]):\n",
    "    ax = axs[ii]\n",
    "    ax.imshow(face, cmap=plt.cm.gray)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines[['right', 'top', \"left\", \"bottom\"]].set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb77b19-456c-43fa-b97c-6bb9b430cd70",
   "metadata": {},
   "source": [
    "(Creepy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab3c621-d546-4be4-9a83-ecc780a63d69",
   "metadata": {},
   "source": [
    "Here's something else can do. Let's build up an image of George W. Bush from his PCA features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d7475-e85a-4354-8ca3-3a9a58a0d227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_GWB = pca.transform(X[6, :].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef58de-ddc5-44f8-9b64-9b8640d7620d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 15, figsize=(10, 1))\n",
    "\n",
    "for ii in range(15):\n",
    "    ax = axs[ii]\n",
    "    \n",
    "    face = w_GWB[:, :(ii+1)*15] @ pca.components_[:(ii+1)*15, :]\n",
    "    \n",
    "    ax.imshow(face.reshape(*image_shape), cmap=plt.cm.gray)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines[['right', 'top', \"left\", \"bottom\"]].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe747d-480c-47f1-b7b7-89bd50db2fd2",
   "metadata": {},
   "source": [
    "Let's now use a simple SVC classifier to try and figure out which face belongs to whom using the decomposed representation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ecd09-64ad-4452-add2-af1abaec12d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_train = pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3998c7-67d4-44a9-b296-aa5ed137310c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa551fa-6ac6-459b-b6af-0121a6521884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"rbf\", class_weight=\"balanced\", C=76823, gamma=0.0034)  # Best hyperparamerters from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da80921-bc3a-4d93-b00c-99d2f3192fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.fit(w_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443182a2-5b38-48bd-be91-ef83e940a75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ec955-24a8-43a1-b8c0-2a18ecc4f071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_test = pca.transform(X_test)\n",
    "y_pred = clf.predict(w_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    clf, w_test, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21bfa2-dc85-4826-9411-eb2bd2d33ef1",
   "metadata": {},
   "source": [
    "# Real research example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bd715-4fc9-4700-9636-f76c9c9e4265",
   "metadata": {},
   "source": [
    "We now follow along with the data in Torrisi _et al_ to show a way in which PCA can be used in real research. The data we pull below is available [open access](https://data.matr.io/4/).\n",
    "\n",
    "S. B. Torrisi, M. R. Carbone, B. A. Rohr, J. H. Montoya, Y. Ha, J. Yano, S. K. Suram & L. Hung. [Random forest machine learning models for interpretable X-ray absorption near-edge structure spectrum-property relationships.](https://www.nature.com/articles/s41524-020-00376-6) npj Comput. Mater. 6, 109 (2020).\n",
    "\n",
    "See also a few of my and my BNL colleagues' works, all of which use PCA or some other forms of dimensionality reduction:\n",
    "\n",
    "- M. R. Carbone, S. Yoo, M. Topsakal & D. Lu. [Classification of local chemical environments from x-ray absorption spectra using supervised machine learning](https://doi.org/10.1103/PhysRevMaterials.3.033604). Physical Review Materials 3, 033604 (2019).\n",
    "- M. R. Carbone, M. Topsakal, D. Lu & S. Yoo. [Machine-learning X-ray absorption spectra to quantitative accuracy](https://doi.org/10.1103/PhysRevLett.124.156401). Physical Review Letters 124, 156401 (2020).\n",
    "- E. J. Sturm, M. R. Carbone, D. Lu, A. Weichselbaum & R. M. Konik. [Computing Anderson Impurity Model Spectra Using Machine Learning](https://doi.org/10.1103/PhysRevB.103.245118). Physical Review B 103, 245118 (2021).\n",
    "- C. Miles, M. R. Carbone, E. J. Sturm, D. Lu, A. Weichselbaum, K. Barros & R. M. Konik. [Machine learning of Kondo physics using variational autoencoders and symbolic regression](https://doi.org/10.1103/PhysRevB.104.235111). Physical Review B 104, 235111 (2021).\n",
    "- A. Ghose, M. Segal, F. Meng, Z. Liang, M. S. Hybertsen, X. Qu, E. Stavitski, S. Yoo, D. Lu & M. R. Carbone. [Uncertainty-aware predictions of molecular X-ray absorption spectra using neural network ensembles](https://doi.org/10.1103/PhysRevResearch.5.013180). Physical Review Research 5, 013180 (2023).\n",
    "- Z. Liang, M. R. Carbone, W. Chen, F. Meng, E. Stavitski, D. Lu, M. S. Hybertsen & X. Qu. [Decoding Structure-Spectrum Relationships with Physically Organized Latent Spaces](https://doi.org/10.1103/PhysRevMaterials.7.053802). Physical Review Materials 7, 053802 (2023).\n",
    "\n",
    "First, we have to get the data. To do this, we use the `requests` module to directly pull the content of the webpage, and then parse that specific format (which despite the extension is not exactly JSON). It is not important to understand the particulars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e874571-bafe-42da-8da0-cf4810f57ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595573ec-f6c4-4e7f-ad28-792e3d1b073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s3.amazonaws.com/publications.matr.io/4/deployment/data/files/spectral_data/Ti_XY.json\"\n",
    "r = requests.get(url)\n",
    "text = r.text.split(\"\\n\")\n",
    "data = [json.loads(xx) for xx in text[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea4d06-f3bb-46e8-8217-b3b44fc18cbd",
   "metadata": {},
   "source": [
    "Get the inputs and outputs from this list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161e1c6-f3ae-4433-a98b-1f4cb73134c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_grid = data[0][\"E\"]\n",
    "spectra = np.array([\n",
    "    dat[\"mu\"] for dat in data\n",
    "    if dat[\"one_hot_coord\"] is not None\n",
    "])\n",
    "coordinations = np.array([\n",
    "    dat[\"coordination\"] for dat in data\n",
    "    if dat[\"one_hot_coord\"] is not None\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b07d9-3e9c-4831-bbfb-9d167d7544f9",
   "metadata": {},
   "source": [
    "The labels, `coordinations` are the the coordination number of an X-ray-absorbing atom! If you don't know what this means, don't worry too much about it. Let's say the task at hand is that we're simply trying to classify whether or not an X-ray absorption spectrum can be used to predict this number. We can use PCA as an indicator of how well a machine learning model may perform on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3ab7a-949d-4fe0-aa7b-32e9f2bd9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(coordinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce811530-c641-4f13-b0d5-6ab20137d954",
   "metadata": {},
   "source": [
    "Here are what some of the spectra look like. These are our input features, while the classes above are our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399bbda-a374-4be1-b4c8-a98e14bf5f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2, 1))\n",
    "\n",
    "for spec in spectra[::50]:\n",
    "    ax.plot(e_grid, spec, color=\"black\", alpha=0.1)\n",
    "\n",
    "ax.set_ylabel(\"$\\mu(E)$ / a.u.\")\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"$E$ / e.V.\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b20bf1-e9ab-4d44-ab61-9fdcfaa5e744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "pca = PCA(n_components=n_components).fit(spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f7e65-dc33-4ef3-8c36-857d71a3465b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = pca.transform(spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d6479-a1c2-41be-8cc4-e9549850691f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "\n",
    "ax.scatter(w[:, 0], w[:, 1], c=coordinations, s=0.5)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
