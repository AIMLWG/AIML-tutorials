{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Basics\n",
    "\n",
    "Yihui \"Ray\" Ren \n",
    "yren@bnl.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this tutorial, you will learn the bolts and nuts of PyTorch. From the basics such as `torch.Tensor`, to something unique for deep learning packages such as `autograd` and `torch.Module`. If we have more time, we will give it a try on using GPUs with PyTorch and demostrate how fast and how easy it is.\n",
    "\n",
    "The goal of this tutorial is to teach you the basics of PyTorch, rather than some fancy deep learning models, which you will learn in later tutorials. So, let's keep the ML model simple, and see how you can implement the same model, a linear regression model, in different ways as you learn each feature of PyTorch. In the same sprit of simplicity, we are not worrying about train validation split in this tutorial.\n",
    "\n",
    "## TOC:\n",
    "\n",
    "* Vectorized Computation\n",
    "    - numpy torch interchangable API\n",
    "    - simple linear regression in numpy and torch\n",
    "* AutoGrad (Automatic Differentiation)\n",
    "    - torch tensor, backward and `grad` \n",
    "    - autograd demo\n",
    "    - `torch.Module` and `forward`.\n",
    "    - re-write linear regression in `torch.Module`\n",
    "* Handling Data \n",
    "    - Stochastic Gradient Descent (SGD)\n",
    "    - `torch.DataSet`\n",
    "    - `torch.DataLoader`\n",
    "    - re-re-write linear regression in `torch.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load modules\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "!date +%D\n",
    "for pkg in [\"np\", \"pd\", \"torch\"]:\n",
    "    print(f\"{pkg:<6} ver: {eval(pkg).__version__}\")\n",
    "\n",
    "# as of last test 12/13/21\n",
    "# np     ver: 1.20.3\n",
    "# pd     ver: 1.3.3\n",
    "# torch  ver: 1.9.1\n",
    "\n",
    "# also tested on colab \n",
    "# 12/13/21\n",
    "# np     ver: 1.19.5\n",
    "# pd     ver: 1.1.5\n",
    "# torch  ver: 1.10.0+cu111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some boilplate functions for display\n",
    "\n",
    "def line_breaker(foo=None):\n",
    "    return f\"\"\"{\"=\"*30}{foo if foo else \"=\"*20:^20}{\"=\"*30}\"\"\"\n",
    "\n",
    "def eval_expr(expr):\n",
    "    try:\n",
    "        print(expr, \":\", eval(expr))\n",
    "    except Exception as e:\n",
    "        print(expr, \"failed because:\", e)\n",
    "\n",
    "def plt_linear_fit(x, y, yhat, a, b):\n",
    "    \"\"\"\n",
    "        x, y: input and groud truth\n",
    "        yhat: prediction\n",
    "        a, b: parameters of linear model\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1,2,figsize=(8,4), sharey=True)\n",
    "    ax1, ax2 = axes\n",
    "    ax1.scatter(x, y, facecolor='none', edgecolor='b', alpha=0.1)\n",
    "    ax1.plot(np.linspace(0, 30), a*np.linspace(0,30)+b, 'r')\n",
    "    ax1.set_title(f\"a={a:.3E}, b={b:.3E}\")\n",
    "    ax1.set_xlim([0,30])\n",
    "    ax1.set_ylim([0.985,1.01])\n",
    "    ax1.set_ylabel(\"ground truth y\")\n",
    "    ax1.set_xlabel(\" x \")\n",
    "    ax2.scatter(yhat, y, facecolor='none', edgecolor='b', alpha=0.1)\n",
    "    # ax2.set_title(f\"MAE = {np.abs(y-yhat).mean():.3E}\")\n",
    "    ax2.set_title(f\"$R^2$ = {1-np.power(y-yhat, 2).sum()/np.power(y-y.mean(),2).sum():.3}\")\n",
    "    ax2.set_ylim([0.985,1.01])\n",
    "    ax2.set_xlim([0.985,1.01])\n",
    "    ax2.set_xlabel(\"yhat\")\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Computation\n",
    "As for tensor calculation, PyTorch is very similar to numpy. Many functions are very similar. There are blog posts comparing the two, such as [this](https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/) and [that](https://rickwierenga.com/blog/machine%20learning/numpy-vs-pytorch-linalg.html). I will use array, ndarray and tensor interchangibly. If you missed the numpy tutorial, please refer to `001_NumPy.ipynb`.\n",
    "\n",
    "* Array Creation and Converting between Numpy and PyTorch\n",
    "* Tensor Operation: add, multiply, elementwise multiply, and so on \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array Creation\n",
    "some_shape = (5,3)\n",
    "some_list = [5,3,2,1]\n",
    "def compare_numpy_torch(np_cb, th_cb, some):\n",
    "    x = np_cb(some)\n",
    "    y = th_cb(some)\n",
    "    x, y = x.shape, y.shape\n",
    "    return x, y\n",
    "\n",
    "for func in [\"empty\", \"ones\", \"zeros\"]:\n",
    "    print(line_breaker(\"comparing \"+func))\n",
    "    npf, thf = eval(\"np.\"+func), eval(\"torch.\"+func)\n",
    "    print(compare_numpy_torch(npf, thf, some_shape))\n",
    "\n",
    "# random tensor\n",
    "print(line_breaker(\"comparing \"+\"rand\"))\n",
    "x = np.random.rand(*some_shape) # np rand does not take a tuple for shape\n",
    "y = torch.rand(some_shape)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "## change random seed, get and set state\n",
    "np.random.seed(5)\n",
    "rng_state = np.random.get_state()\n",
    "np.random.set_state(rng_state)\n",
    "\n",
    "torch.random.manual_seed(5)\n",
    "rng_state = torch.random.get_rng_state()\n",
    "torch.random.set_rng_state(rng_state)\n",
    "\n",
    "# convert between numpy and torch\n",
    "print(line_breaker(\"convert btwn np and torch\"))\n",
    "x = np.random.rand(2,2)\n",
    "y = torch.tensor(x) # convert np to torch\n",
    "z = y.numpy() # convert torch to np\n",
    "assert (x == z).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matrix operation\n",
    "def init_two_mat(shape):\n",
    "    op = \"init\"\n",
    "    print(line_breaker(op))\n",
    "    npx = np.random.rand(*shape)\n",
    "    thx = torch.tensor(npx)\n",
    "    # print(\"x:\",npx)\n",
    "    return npx, thx\n",
    "\n",
    "x_shape = (3, 3)\n",
    "y_shape = (3, 3)\n",
    "npx, thx = init_two_mat(x_shape)\n",
    "npy, thy = init_two_mat(y_shape)\n",
    "\n",
    "### add \n",
    "op = \"add\"\n",
    "print(line_breaker(op))\n",
    "npz = npx + npy\n",
    "thz = thx + thy\n",
    "print(\"x+y=z:\", npz)\n",
    "assert (npz == thz.numpy()).all()\n",
    "\n",
    "### elementwise mult aka Hadamard product\n",
    "op = \"elementwise multi\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npz = npx*npy\n",
    "thz = thx*thy\n",
    "print(\"x*y=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mat product (nxp) x (pxm) -> (nxm)\n",
    "\n",
    "x_shape = (3, 4)\n",
    "y_shape = (4, 3)\n",
    "npx, thx = init_two_mat(x_shape)\n",
    "npy, thy = init_two_mat(y_shape)\n",
    "\n",
    "op = \"multiply\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npz = npx@npy\n",
    "thz = thx@thy\n",
    "print(\"x@y=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "# alternatively \n",
    "npz = npx.dot(npy) # avoid using this function for its confusing name\n",
    "thz = thx.mm(thy) # only works for matrices\n",
    "print(\"x.mm(y)=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "# operator launched by package name.\n",
    "npz = np.matmul(npx,npy)\n",
    "thz = torch.matmul(thx, thy) \n",
    "print(\"pkg.mm(x, y)=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "\n",
    "### matmul also works for batched matrix mult \n",
    "npx, thx = init_two_mat((3, 1, 4, 5))\n",
    "npy, thy = init_two_mat((6, 5, 7))\n",
    "npz = np.matmul(thx, thy)\n",
    "thz = torch.matmul(thx, thy) \n",
    "print(\"thz shape\", thz.shape)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "assert thz.shape == (3,6,4,7)\n",
    "\n",
    "### will mm, dot, and \"@\" will  work ?\n",
    "eval_expr(\"npx.dot(npy).shape\") # works in strange way\n",
    "eval_expr(\"(npx@npy).shape\") # works as matmul\n",
    "eval_expr(\"(thx@thy).shape\") # works as matmul \n",
    "eval_expr(\"thx.mm(thy).shape\") # cant, must be a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensor Dimension Manipulations\n",
    "def create_test_tensors(x_shape):\n",
    "    npx = np.random.rand(*x_shape)\n",
    "    thx = torch.tensor(npx)\n",
    "    return npx, thx\n",
    "    \n",
    "### transpose\n",
    "op = \"transpose\"\n",
    "print(line_breaker(op))\n",
    "\n",
    "tensor_shape = (1,3)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npxT = npx.T\n",
    "thxT = thx.T\n",
    "\n",
    "tensor_shape = (3,4,5,6)\n",
    "#  dim idx     (0,1,2,3)\n",
    "new_order = \"(1,0,3,2)\"\n",
    "#  dim sz    (4,3,6,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "print(\"before transpose\", npx.shape, thx.shape)\n",
    "eval_expr(f\"np.transpose(npx, {new_order}).shape\")\n",
    "eval_expr(f\"npx.transpose({new_order}).shape\")\n",
    "eval_expr(f\"thx.permute({new_order}).shape\") \n",
    "eval_expr(f\"torch.permute(thx, {new_order}).shape\")\n",
    "\n",
    "# .T will simply reverse the dim order  -> (6,5,4,3)\n",
    "eval_expr(\"npx.T.shape\")\n",
    "eval_expr(\"thx.T.shape\")\n",
    "\n",
    "\n",
    "### flatten and reshape \n",
    "op = \"flatten\"\n",
    "print(line_breaker(op))\n",
    "tensor_shape = (3,4,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npflat1 = npx.reshape(-1)\n",
    "npflat2 = npx.flatten()\n",
    "thflat1 = thx.reshape(-1)\n",
    "thflat2 = thx.flatten()\n",
    "thflat3 = thx.view(-1)\n",
    "thflat4 = torch.flatten(thx)\n",
    "for x in [npflat1, npflat2, thflat1, thflat2, thflat3, thflat4]:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Squeeze and Unsqueeze (adding and removing dummy dimensions)\n",
    "op = \"squeeze\"\n",
    "print(line_breaker(op))\n",
    "tensor_shape = (3,1,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "print(\"before squeeze\", npx.shape, thx.shape)\n",
    "npxs = npx.squeeze()\n",
    "thxs = thx.squeeze() \n",
    "print(\"after squeeze \", npxs.shape, thxs.shape)\n",
    "\n",
    "op = \"unsqueeze\"\n",
    "print(line_breaker(op))\n",
    "npxus = np.expand_dims(npxs,1)\n",
    "thxus = thxs.unsqueeze(1)\n",
    "print(\"after unsqueeze at dim 1:\", npxus.shape, thxus.shape)\n",
    "\n",
    "### Concat \n",
    "op = \"concatenate\"\n",
    "print(line_breaker(op))\n",
    "tensor_shape = (3,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npy, thy = create_test_tensors(tensor_shape)\n",
    "print(\"before concat\", npx.shape, npy.shape)\n",
    "npz0 = np.concatenate((npx, npy), axis=0)\n",
    "thz0 = torch.cat((thx, thy), axis=0)\n",
    "assert npz0.shape == thz0.shape\n",
    "print(\"after concat along dim 0:\", npz0.shape)\n",
    "npz1 = np.concatenate((npx, npy), axis=1)\n",
    "thz1 = torch.cat((thx, thy), axis=1)\n",
    "assert npz1.shape == thz1.shape\n",
    "print(\"after concat along dim 1:\", npz1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using Vectorized Computation\n",
    "\n",
    "* Wine Quality Dataset\n",
    "* Linear regression using a formula\n",
    "* Homework: convert numpy implementation to torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get data for linear regression. \n",
    "## find the most correlated two features for linear regression. \n",
    "wine_quality_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "df = pd.read_csv(wine_quality_url, delimiter=\";\")\n",
    "w = df.corr()\n",
    "up_tri = np.triu(np.abs(w.to_numpy()),k=1)\n",
    "max_idx = np.argmax(up_tri)\n",
    "col_sz = len(df.columns)\n",
    "col1, col2 = df.columns[max_idx//col_sz], df.columns[max_idx%col_sz]\n",
    "# print(max_idx,up_tri.flatten()[max_idx], col1, col2)\n",
    "print(f\"\"\"found \"{col1}\" and \"{col2}\" for linear regression\"\"\") # most correlated two features\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(10,4))\n",
    "htm = sns.heatmap(w, ax=axes[0])\n",
    "htm.set_xticklabels(htm.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "sns.scatterplot(x=df[col1], y=df[col2], ax=axes[1])\n",
    "axes[1].set_title(f\"{col1} and {col2} \\n corr.coef. = {np.corrcoef(df[col1], df[col2])[0,1]:.4f}\")\n",
    "\n",
    "wine_x, wine_y = df[col1].to_numpy(), df[col2].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression \n",
    "Find a and b such that:\n",
    "$\\sum (y-y')^2$\n",
    "is minimized, where\n",
    "$y' = ax+b$\n",
    "\n",
    "$a$ and $b$ are your linear model parameters.\n",
    "\n",
    "with solution \n",
    "\n",
    "$a = \\sum(x - \\bar{x})(y - \\bar{y}) / \\sum (x - \\bar{x})^2$\n",
    "\n",
    "$b = \\bar{y} - a\\bar{x}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n10 -r10\n",
    "## here is a numpy implementation\n",
    "xbar = np.mean(wine_x)\n",
    "ybar = np.mean(wine_y)\n",
    "a = (wine_x - xbar)@(wine_y-ybar).T / np.power(wine_x-xbar, 2).sum()\n",
    "b = ybar - a * xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_linear_fit(wine_x, wine_y, a*wine_x+b, a, b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: re-write above code in torch\n",
    "here is the torch doc https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thx = torch.tensor(wine_x)\n",
    "thy = torch.tensor(wine_y)\n",
    "##                            ##\n",
    "##                            ##\n",
    "##  Intentionally Left Blank  ## \n",
    "##                            ##\n",
    "##                            ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n10 -r10\n",
    "## Solution\n",
    "## comparing to the numpy implementation line by line, do you spot any difference?\n",
    "thx = torch.tensor(wine_x)\n",
    "thy = torch.tensor(wine_y)\n",
    "xbar = torch.mean(thx)\n",
    "ybar = torch.mean(thy)\n",
    "a = (thx - xbar)@(thy-ybar).T / torch.pow(thx-xbar, 2).sum()\n",
    "b = ybar - a * xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_linear_fit(wine_x, wine_y, (a*thx+b).numpy(), a.numpy(), b.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad\n",
    "In this section, you will learn automated differentiation.\n",
    "1. Torch tensor has built-in grad\n",
    "    - `x.data`\n",
    "    - `x.grad`\n",
    "    - `x.backward()`\n",
    "2. Wrap tensors and operations into `nn.Module` so they are chained \n",
    "    - `nn.Module`\n",
    "    - `forward()`\n",
    "    - `torch.Parameter()`\n",
    "3. Revisit Linear Model   \n",
    "    - `nn.Linear`\n",
    "    - Gradiant Descent\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Tensor Autograd\n",
    "image credit: https://datahacker.rs/004-computational-graph-and-autograd-with-pytorch/\n",
    "![image](https://media5.datahacker.rs/2021/01/11-1024x593.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## y = a*x, dy/dx = ? \n",
    "x = torch.tensor(0.1, requires_grad=True)\n",
    "a = torch.tensor(3) # not requires grad\n",
    "y = a*x # y=ax, dy/dx = a\n",
    "y.backward()\n",
    "print(\"variable x\", x.data, \"has grad of\", x.grad)\n",
    "# what is a.grad ? will it still work if `a = 3`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here is another example\n",
    "## y = exp(a*x), dy/dx = ? \n",
    "## see if this is what you expected.\n",
    "x = torch.tensor(0.3, requires_grad=True)\n",
    "a = torch.tensor(3)\n",
    "y = torch.exp(a*x) \n",
    "# do it by hand using chain rule: \n",
    "# y = exp(ax), dy/dx = exp(ax) d(ax)/dx = a * exp(ax) \n",
    "print(\"do it by hand:\\n\", \"y = exp(ax), dy/dx = exp(ax) d(ax)/dx = a * exp(ax)\")\n",
    "print(\"before backward(), x grad is: \", x.grad)\n",
    "y.backward()\n",
    "print(\"variable x\", x, \"has grad of\", x.grad, \"which should be the same as\", a*torch.exp(a*x).detach())\n",
    "## Bonus question: what if you backward() again? and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: pick a f(x) you like, and autograd it!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##                            ##\n",
    "##                            ##\n",
    "##  Intentionally Left Blank  ## \n",
    "##                            ##\n",
    "##                            ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider this function: $ y = ( \\sin x + 1 )^x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution:  \n",
    "## ok, another example y = (sin(x)+1)^x \n",
    "print(\"for example y = (sin(x)+1)^x\")\n",
    "x = torch.tensor(0.2, requires_grad=True)\n",
    "y = (torch.sin(x)+1).pow(x)\n",
    "y.backward()\n",
    "ans = torch.exp(x*torch.log(torch.sin(x)+1))*((torch.log(torch.sin(x)+1))+x*(torch.sin(x)+1).pow(-1)*torch.cos(x))\n",
    "print(x, x.grad, ans)\n",
    "\n",
    "## summary: autograd works like a charm\n",
    "## bonus: see how you can define your own auto-grad function that is not obviously defirentiable:\n",
    "##  https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html#pytorch-defining-new-autograd-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Torch Module \n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "# torch.module: Packing parameters and functions together\n",
    "# motivations: for each composition and recursive \"apply\"\n",
    "# two APIs:\n",
    "# __init__() and forward()\n",
    "\n",
    "class TenFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(0.2, requires_grad=True) \n",
    "        \n",
    "    def forward(self):\n",
    "        # from above exapmle y = (sin(x)+1)^x \n",
    "        # usually forward takes an input, omit it for now.\n",
    "        return (self.x.sin()+1).pow(self.x)\n",
    "    \n",
    "## create a module \n",
    "func = TenFunc()\n",
    "y = func()\n",
    "y.backward()\n",
    "print(func.x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error: if requires_grad=False\n",
    "print(\"Warning: this will produce error\")\n",
    "class FuncError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(0.2) \n",
    "        \n",
    "    def forward(self):\n",
    "        return (self.x.sin()+1).pow(self.x)\n",
    "    \n",
    "## create a module \n",
    "func = FuncError()\n",
    "y = func()\n",
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as err:\n",
    "    print(err)\n",
    "    \n",
    "print(\"func.x.grad is\", func.x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better to use torch.nn.Parameter\n",
    "## and rename x as w (as weights) as a convention.\n",
    "\n",
    "class ParaFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        w = torch.tensor(0.2)\n",
    "        self.w = nn.Parameter(w)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (self.w.sin()+1).pow(self.w)\n",
    "    \n",
    "## create a module \n",
    "func = ParaFunc()\n",
    "y = func(None)\n",
    "y.backward()\n",
    "print(func.w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [Optional] (requires GPU)\n",
    "## The benefits of using Parameter are two folds: \n",
    "#  * registered to module parameters. \n",
    "#  * moves with modules to device.\n",
    "\n",
    "print(line_breaker())\n",
    "print(\"nn.Parameters are registered to nn.Module\")\n",
    "func = ParaFunc()\n",
    "for p in func.parameters():\n",
    "    print(p)\n",
    "    \n",
    "print(line_breaker())\n",
    "print(\"torch.tensor was not registered\")\n",
    "func = TenFunc()\n",
    "for p in func.parameters():\n",
    "    print(p)\n",
    "print(\"got nothing\")\n",
    "print(line_breaker())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(line_breaker(\"move btwn cpu&gpu\"))\n",
    "    print(\"registered parameter moves with Module\")\n",
    "    func_p = ParaFunc()\n",
    "    print(f\"before move: \\t{func_p.w.device}\")\n",
    "    # also works on GPU\n",
    "    func_p = func_p.cuda()\n",
    "    print(f\"after move: \\t{func_p.w.device}\")\n",
    "    \n",
    "    print(\"unregistered tensor does not\")\n",
    "    # if Func tensor\n",
    "    func_t = TenFunc()\n",
    "    print(f\"before move: \\t{func_t.x.device}\")\n",
    "    # also works on GPU\n",
    "    func_t = func_t.cuda()\n",
    "    print(f\"after move: \\t{func_t.x.device}\")\n",
    "    \n",
    "    ## print out:\n",
    "    #  cpu\n",
    "    #  cuda:0\n",
    "    #  cpu\n",
    "    #  cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Composition\n",
    "Modules can be composed together or called in sequence, auto-grad will work in both cases.\n",
    "Let's still use this function: $ y = ( \\sin x + 1 )^x $, and break it into two simpler ones $\\sin(x)$ and $u^x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modules can be composed and auto grad works \n",
    "## let's break the previous function into two parts, \n",
    "class SinFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "    \n",
    "class PowFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input, x):\n",
    "        return torch.pow(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(0.2, requires_grad=True)\n",
    "f = SinFunc()\n",
    "g = PowFunc()\n",
    "y = g(f(x)+1, x)\n",
    "y.backward()\n",
    "x.grad  # recall the correct answer is tensor(0.3575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Modules within Module\n",
    "        self.f = SinFunc()\n",
    "        self.g = PowFunc()\n",
    "    def forward(self, x):\n",
    "        return self.g(self.f(x)+1, x)\n",
    "h = CombineFunc()\n",
    "x = torch.tensor(0.2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = h(x)\n",
    "y.backward()\n",
    "x.grad\n",
    "# what's the value of x.grad? why is it? what if you run several times?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit Linear Model\n",
    "Let's get back to the linear regression model. We can solve it another way using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent): $a_{n+1} = a_n - \\gamma \\nabla F(a_n)$\n",
    "![image](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png)\n",
    "image credit: http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.random.manual_seed(7) \n",
    "        # some random guess of initial values\n",
    "        self.a = nn.Parameter(torch.rand(1), requires_grad=True) \n",
    "        self.b = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.a + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thx = torch.tensor(wine_x)\n",
    "thy = torch.tensor(wine_y)\n",
    "lin = MyLinearModel()\n",
    "yhat = lin(thx)\n",
    "V = lambda x: x.cpu().detach().squeeze().numpy()  #  tensor to numpy for easy plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.003 # learning rate \\gamma\n",
    "num_epochs = 10**4\n",
    "for ep in range(num_epochs):\n",
    "    yhat = lin(thx)\n",
    "    loss = ((thy-yhat)*(thy-yhat)).mean()\n",
    "    loss.backward()\n",
    "    for p in lin.parameters():\n",
    "        # Q: what is p? \n",
    "        p.data -= lr * p.grad\n",
    "        p.grad.zero_() \n",
    "        # Q: recall why we need to set grad to zero?\n",
    "    \n",
    "    if ep%10**3==0:\n",
    "        clear_output()\n",
    "        plt.close(fig)\n",
    "        fig = plt_linear_fit(wine_x, wine_y, V(lin(thx)), V(lin.a), V(lin.b))\n",
    "        display(fig)\n",
    "        time.sleep(1)\n",
    "\n",
    "for name, p in lin.named_parameters():\n",
    "    print(name, \"has value\", p.data)\n",
    "\n",
    "## bonous: learning rate and number of epochs are chosen for you in this example. \n",
    "##         They are called \"Hyper-parameters\"\n",
    "##         Try to change their values, and explain how they impact the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Data\n",
    "\n",
    "In most cases, we are facing a very large dataset and a very large model. So large that an ordinary memory will not fit everything. SGD treats a random subset of the dataset as an \"entire dataset\" for one GD step. Such random subset at every step is called \"mini batch\"\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- `torch.DataSet`\n",
    "- `torch.DataLoader`\n",
    "- `torch.nn.MSELoss`\n",
    "- `torch.optim.SGD`\n",
    "- `torch.nn.Linear` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.Dataset`\n",
    "`Dataset` has two API: `__getitem__` and `__len__`\n",
    "if you are a veteran of python, you know they give your \n",
    "class object x `x[idx]` operator  and \n",
    "`len(x)` to query the total size.\n",
    "\n",
    "now, let's pack our wine dataset into the `Dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class WineData(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.astype(np.single)\n",
    "        self.y = y.astype(np.single)\n",
    "        assert len(self.x) == len(self.y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "# usage example:\n",
    "dataset = WineData(wine_x, wine_y)\n",
    "print(\"data sz = \", len(dataset))\n",
    "print(\"get the 3rd item\",dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "`DataLoader` wraps data into a generator. (`__iter__` and `__next__`)\n",
    "One can iterate the dataset subset by subset of `batch_size`.\n",
    "At the end of iteration, the sequence of data is shullfled (\"stochastic\"), if one sets `shuffle=True`.\n",
    "It also provides a way to split the data loading workload among multiple CPU workers via `num_workers=4`.\n",
    "This will be useful when each data point is very large and requires expensive pre-processing.\n",
    "\n",
    "read more: https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some use examples:\n",
    "dataset = WineData(wine_x, wine_y)\n",
    "dataloader = DataLoader(dataset, batch_size=4, drop_last=True, shuffle=True)\n",
    "print(next(iter(dataloader))) \n",
    "# notice that x and y are grouped together, \n",
    "# as x will go through the model, and y will be used in the loss function\n",
    "\n",
    "# next minibatch\n",
    "x, y  = next(iter(dataloader))\n",
    "print(x.data)\n",
    "print(y.data)\n",
    "\n",
    "for (x, y), i in zip(dataloader, range(5)):\n",
    "    print(i,\":\",x.data,y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "The loss function is usually the last numeric operation in\n",
    "the computation graph. It calculates how far the current\n",
    "model from the target (ground truth). There are many common\n",
    "loss functions built-in in the PyTorch. And it is not so\n",
    "difficult to write your own, since under the hood, they are\n",
    "nothing but `nn.Module`s. \n",
    "\n",
    "read more on loss functions: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recall our loss function is (y-yhat)^2, this is called mean squared error or MSE\n",
    "## note `thy` and `yhat` from previous linear model, one should see very small loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "prev_loss_fn = ((thy-yhat)*(thy-yhat)).mean()\n",
    "print(loss_fn(thy,yhat))\n",
    "print(prev_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "SGD is part of PyTorch Optimizer package: `torch.optim`. To initialize an optimizer, we need to pass the parameters of the model and some hyper-parameters such as learning rate.\n",
    "\n",
    "The optimizer, say `optmzr`, does two things: \n",
    "- `optmzr.zero_grad()` sets all parameters' gradients to \n",
    "   zero (recall `recursive apply` and why we use `nn.Module`), and\n",
    "- `optmzr.step()` to adjust all parameters `.data` by the amoung of\n",
    "   `.grad` weighted by the step size (or learning rate in the case of SGD).\n",
    "\n",
    "SGD is the most classic learning algorithm where the step size is fixed and determined by the learning rate. There are many more advanced learning algorithms that the step size is adjusted on the fly.\n",
    "\n",
    "read more: https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## usage example\n",
    "model = MyLinearModel()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "model.a.grad = torch.tensor([0.5]) ## let's manuall set grad of a to 0.5\n",
    "print(model.a.data, model.a.grad)\n",
    "optim.zero_grad()\n",
    "print(\"after zero_grad\", model.a.data, model.a.grad)\n",
    "\n",
    "model.a.grad = torch.tensor([0.4]) ## let's manuall set grad of a to 0.4\n",
    "print(model.a.data, model.a.grad)\n",
    "optim.step() # Question, what is the value `model.a.data`? lr=0.5, grad=0.4\n",
    "print(model.a.data, model.a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Linear`\n",
    "\n",
    "PyTorch has many neural network models and model components. `nn.Linear` is exactly our linear model.\n",
    "Recall that a linear model is nothing but $ax+b$ where $a$ is called weight and $b$ is called bias.\n",
    "To initialize a `Linear` model, we simply put the `in_features` and `out_features`.\n",
    "\n",
    "- `nn.Linear(in_features=3, out_features=5)` will result a weight matrix (5x3), \n",
    "- `model.weight` and `model.bias` to access the model's weight and bias\n",
    "\n",
    "\n",
    "read more: https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3,5)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together\n",
    "\n",
    "1. dataset\n",
    "1. dataloader\n",
    "1. loss function\n",
    "1. model\n",
    "1. optimizer\n",
    "1. train in a for loop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineData(wine_x, wine_y)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "loss_fn = nn.MSELoss()\n",
    "model = nn.Linear(1,1)\n",
    "optmzr = torch.optim.SGD(model.parameters(), lr=0.003)\n",
    "num_epochs = 1000\n",
    "for e in range(num_epochs):\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.unsqueeze(1), y.unsqueeze(1)\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(y, yhat)\n",
    "        \n",
    "        # update the weights\n",
    "        optmzr.zero_grad()\n",
    "        loss.backward()\n",
    "        optmzr.step()\n",
    "    \n",
    "    if e%100==0:\n",
    "        clear_output()\n",
    "        plt.close(fig)\n",
    "        ypred = model(thx.clone().detach().float().unsqueeze(1))\n",
    "        fig = plt_linear_fit(wine_x, wine_y, V(ypred), V(model.weight.data), V(model.bias))\n",
    "        display(fig)\n",
    "        time.sleep(1)\n",
    "        \n",
    "print(model.weight.data, model.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our previous gradient descent code, and compare with above SGD code. \n",
    "Can you identify the corresponding part of the code?\n",
    "```\n",
    "lr = 0.003 # learning rate \\gamma\n",
    "num_epochs = 10**4\n",
    "for ep in range(num_epochs):\n",
    "    yhat = lin(thx) \n",
    "    loss = ((thy-yhat)*(thy-yhat)).mean()\n",
    "    loss.backward()\n",
    "    for p in lin.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad.zero_() \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
