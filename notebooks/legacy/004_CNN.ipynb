{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4:  Using CNN for Image Classification\n",
    "\n",
    "## Human Vision\n",
    "\n",
    "In 1950, neurophysiology's David Hunter & Torsten Wiesel, at the John Hopkins School of Medicine, [theorized](https://www.cell.com/neuron/pdf/S0896-6273(00)80984-8.pdf) the existence of two types of cells in the human visual cortex that had something to do with the way we perceived patterns:\n",
    "\n",
    "- __Simple Cells__ are cells that respond to edges of images of random orientation\n",
    "- __Complex Cells__ can perform at the same level as the Simple Cells, but can do so anywhere in the image making this cell prone to Spatial Invariance\n",
    "\n",
    "__Spatial Invariance__ is a property whereby the location of the “object of interest” does not affect the process our minds as well as machine leanring models as we’ll later in this tutorial.\n",
    "__Complex cells__ are aided by simple cells via the summation of the output of several simple cells in different locations within the scope of ones vision. The concept of complex cells being comprised of the sum of simple cells renders these complex cells the __ability to detect features practically anywhere__ (i.e. spatially invariant) within the field of view.\n",
    "\n",
    "This inspired the developement of [neocognitron](https://en.wikipedia.org/wiki/Neocognitron) which is one of the first multilayered artificial neural network. This led to later developement of Convolutional Neural Networks and SIFT (Scale Invariant Feature Transform).\n",
    "\n",
    "## Computer Vision \n",
    "\n",
    "__Computer Vision (CV) is about extracting meaning out of an image/sequence of images (videos), making sense of a picture, and basically helping the computer see!__ Image Processing (ImgProc) should not be mixed with CV because ImgProc is mainly concerned with transforming images via many operations such as blurring, thresholding, and smoothing rather than extracting content or meaning from them. These operations are often used as part of CV algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are an important tool for most machine learning practioners today. Following shows training a CNN model for Image classification task.\n",
    "\n",
    "They are a subset of deep neural networks that exhibit the important trait of Spatial Invariance through kernel convolutions, which in our case is a simple dot product between two matrices, when it comes to Image Classification, and other applications (like Image Segmentation, Object Detection, Object Classification).\n",
    "\n",
    "## Usefulness provided by CNNs\n",
    "\n",
    "- __Parameter Sharing:__ Parameter sharing is an observation whereby we can use a single kernel/filter, with parameters consisting of its weights, to convolve over the same image multiple times effectively reducing the number of parameters you have to deal with per image, per kernel. In other words, __a filter used to convolve over a specific part of the image can also be useful in other parts of the image.__\n",
    "- __Sparsity of Connections:__ Sparsity of connections is another observation whereby a single pixel in a feature map output, which is the product of convolving a kernel filter over a source image matrix, corresponds to a set of n x n pixels in the image, convolved with a filter, thus reducing number of connections images have with respect to the output (i.e. __1 Feature Map pixel ~ n x n set of image pixels convolved with the filter of size n__)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical CNN Architecture\n",
    "\n",
    "CNN image classifications takes an input image, process it and classify it under certain categories (Eg., Dog, Cat, Tiger, Lion). Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ). Eg., An image of 6 x 6 x 3 array of matrix of RGB (3 refers to RGB values) and an image of 4 x 4 x 1 array of matrix of grayscale image.\n",
    "\n",
    "Each input image will pass it through a series of convolution layers with filters (Kernals), Pooling, fully connected layers (FC) and apply Softmax function to classify an object with probabilistic values between 0 and 1. \n",
    "\n",
    "![CNN](data/images/17_cnn_architecture.png)\n",
    "\n",
    "__The figure shows a complete flow of information in a CNN architecture to process an input image and classify based on values.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Learning\n",
    " - __Convolution Operation:__\n",
    " \n",
    " Convolution is the first layer to extract features from an input image. Convolution preserves the relationship between pixels by learning image features using small squares of input data. Small squares are referred to as __filters__ or __kernels__.  It is a mathematical operation that takes two inputs such as image matrix and a filter or kernel. \n",
    " \n",
    " <img src=\"https://www.freecodecamp.org/news/content/images/2019/07/convSobel.gif\" />\n",
    "  \n",
    " __commonly used filters:__\n",
    " \n",
    " <img src=\"./data/images/cnn_filters.png\" />\n",
    "\n",
    "__“convolving” allows the filter to extract what “it” thinks is an important feature, dictated by its weights__\n",
    "\n",
    "__Multi-Channel Convolutions__: Above gifs show convolution operation on a single channel input matrix or image. When dealing with RGB images there are 3 channels (R, G, B). Therefore, a filter with 3 separate channels, one to capture details for each channel in the image is used. \n",
    "\n",
    "Once each channel in the filter has convolved over its appropriate channel in the image, an intermediate step is required whereby the results of the convolutions are simply added to get an overall result.\n",
    "\n",
    " <img src=\"./data/images/rgb_convolutions.gif\" />\n",
    "\n",
    "\n",
    "__Using Multiple filters for edge detection in RGB images__\n",
    "\n",
    " <img src=\"./data/images/multi_edge_filter.png\" />\n",
    "\n",
    "\n",
    " - __Stride:__ \n",
    " \n",
    " Stride is the number of pixels shifts over the input matrix. When stride is 1 then filter moves by 1 pixel. When stride is 2 the filter moves by 2 pixels and so on.\n",
    " \n",
    " - __Padding:__\n",
    " \n",
    " __The Border-Effect__ is a phenomenon whereby the n x n filter cannot capture the outer rows and columns of the image simply because a part of the filter would extend beyond the image where no pixels exist. This leads to information loss. It is solved by adding padding that introduces pixels of zero value all around the input image like shown below. This gives a resulting feature map that is equal to the image itself.  \n",
    " \n",
    " <img src=\"./utils/padding.gif\" alt=\"Padding\" title=\"Applying Padding Feature map = Input\" />\n",
    "\n",
    " - __ReLU:__\n",
    "\n",
    " Rectified Linear Unit (ReLU) is one type of activation function defined as $ f(x) = max(0,x) $ that induces non-linearity to the inputs such that the output is not necessarily a linear combination of the inputs and weights. There are other activation functions like tanh, sigmoid, leakyReLU\n",
    "\n",
    " - __Max Pooling:__\n",
    " \n",
    " Pooling is a technique whereby we can down-sample feature maps, getting rid of any pixels that don’t necessarily represent the feature that we want to extract and still have the down-sampled map representative of the original map. It is also used to reduce the amount of parameters, and hence computations, needed throughout a CNN. Max Pooling returns the maximum value contained within the pool of values in the matrix superimposed on the image/feature.\n",
    " \n",
    " <img src=\"https://miro.medium.com/max/1204/0*uYmuSGuwM7N-88QT.gif\" />\n",
    "\n",
    "     \n",
    "## Classification\n",
    "\n",
    " - __Fully Connected Layer__\n",
    " \n",
    " After extracting meaningful features using steps above the feature map is flatten into a 1-D vector which is assumed to be fully representative of the original input image. This vector is an input to a Fully Connected layer which is essentially learning non-linear combinations of these features.\n",
    "\n",
    " - __Softmax__\n",
    " \n",
    " The Softmax is a form of logistic regression that normalizes an input value into a vector of values that follows a probability distribution whose total sums up to 1.  \n",
    " \n",
    " $ \\sigma(\\vec{z})_{i} = e^{z_{i}} / \\sum_{j=1} ^{K} e^{z_{j}} $\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "## Forward Pass and Backward Pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![FWD_BACK PASS](https://drive.google.com/uc?id=1dgVg08M02gfIkPm0JKyhm4k4gYVLB_Me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification\n",
    "\n",
    "Using MNIST dataset.\n",
    "MNIST (Modified National Institute of Standards and Technology) dataset is like the hello-world in computer vision. The dataset was\n",
    "created way back in the late 90s. The official description states,\n",
    "\n",
    "\"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\"\n",
    "\n",
    "<img src=\"./data/images/mnist_samples.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={}\n",
    "kwargs={}\n",
    "args['batch_size']=1000\n",
    "args['test_batch_size']=1000\n",
    "args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset. \n",
    "args['lr']=0.01 #Learning rate is how fast it will decend. \n",
    "args['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n",
    "\n",
    "args['seed']=1 #random seed\n",
    "args['log_interval']=10\n",
    "args['cuda']=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# MNIST can be directly used from pytorch's library interface (https://pytorch.org/vision/stable/_modules/torchvision/datasets/mnist.html)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model \n",
    "\n",
    "Conv2d : torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    " - in_channels (int) – Number of channels in the input image\n",
    " - out_channels (int) – Number of channels produced by the convolution\n",
    " - kernel_size (int or tuple) – Size of the convolving kernel\n",
    " - stride (int or tuple, optional) – Stride of the convolution. (Default: 1)\n",
    " - padding (int or tuple, optional) – Zero-padding added to both sides of the input (Default: 0)\n",
    " - padding_mode (string, optional) – zeros\n",
    " \n",
    " Input : (N, $C_{in}$, $H_{in}$, $W_{in}$)\n",
    " \n",
    " Output : (N, $C_{out}$, $H_{out}$, $W_{out}$)\n",
    "\n",
    "$ out(N_{i}, C_{out_{j}}) = bias(C_{out_{j}}) + \\sum_{k=0} ^{C_{in} - 1} weight(C_{out_{j}}) * input(N_{i}, k) $\n",
    "\n",
    "__Input/Output dimensions calculation for each Layer in CNN__\n",
    "\n",
    "__conv1:__ Input image is $1x28x28$. Applying $5x5$ filter and $10$ feature maps, the $H$ and $W$ in the output of conv1 is calculated as, \n",
    "\n",
    "$ floor((28 + 2*0 - 1*(5-1) - 1)/2 + 1) $ , Refer [ $ H_{out}, W_{out} $formula](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    "The dividing by $2$ is due to the pooling kernel size of 2 in __forward()__. This gives Output dims = $10x12x12$\n",
    "\n",
    "__conv2:__ Input feature map is $10x12x12$. Applying $5x5$ filter and $20$ feature maps, the $H$ and $W$ in the output of conv2 is calculated as, \n",
    "\n",
    "$ floor((12 + 2*0 - 1*(5-1) - 1)/2 + 1) $ , Refer [ $ H_{out}, W_{out} $formula](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    "The dividing by $2$ is due to the pooling kernel size of $2$ in __forward()__. This gives Output dims = $20x4x4$\n",
    "\n",
    "__fc1:__ Input feature map is a 1-D vector of size $(,20*4*4)$ = $(,320)$ and Output dims is $(,50)$.\n",
    "\n",
    "__fc2:__ Input feature map is of size $(,50)$ produced output vector of classes of dims $(,10)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    #This defines the structure of the NN.\n",
    "    # Conv2d : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    # Dropout2d : https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html\n",
    "    # Linear : https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # Input dims: (N, 1, 28, 28), Output dims: (N, 10, 12, 12)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # Input dims: (N, 10, 12, 12), Output dims: (N, 20, 4, 4)\n",
    "        self.conv2_drop = nn.Dropout2d()  # Dropout Regularization\n",
    "        self.fc1 = nn.Linear(320, 50) # Input dims: (N, 20 * 4 * 4), Output dims: (N, 50)\n",
    "        self.fc2 = nn.Linear(50, 10) # Input dims: (N, 50), Output dims: (N, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Convolutional Layer/Pooling Layer/Activation\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        #Convolutional Layer/Dropout/Pooling Layer/Activation\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        #Fully Connected Layer/Activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training) #default probability is p=0.5, and it is enabled only during training\n",
    "        #Fully Connected Layer/Activation\n",
    "        x = self.fc2(x)\n",
    "        #Softmax gets probabilities. \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    tot_loss = 0\n",
    "    model.train()\n",
    "    print(len(train_loader))\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #Variables in Pytorch are differenciable. \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #This will zero out the gradients for this batch. \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Calculate the loss The negative log likelihood loss. \n",
    "        loss = F.nll_loss(output, target)\n",
    "        #dloss/dx for every Variable \n",
    "        loss.backward()\n",
    "        #to do a one-step update on parameter.\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print out the loss periodically. \n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: NA'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "            \n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False) #.data[0] # sum up batch loss\n",
    "        #print(test_loss)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        #print(pred)\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "if args['cuda']:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    print(epoch)\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# Plots are plotted inside the notebooks, 'inline'\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "pred = None\n",
    "dataset = None\n",
    "for data, target in test_loader:\n",
    "    dataset = data\n",
    "    if args['cuda']:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    \n",
    "    test_loss += F.nll_loss(output, target, size_average=False) #.data[0] # sum up batch loss\n",
    "    \n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    \n",
    "    correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "figure = plt.figure() #figsize=(10, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows +1): #range(4):\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.imshow(dataset[i-1].numpy().reshape((28,28)).squeeze(), cmap='gray')\n",
    "    label = np.asarray(pred[i-1].data[0].cpu())\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "figure.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_tutorials",
   "language": "python",
   "name": "aiml_tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
