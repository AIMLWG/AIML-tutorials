{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bdd72-3106-4560-8f4c-dfc09c419f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90a17c-eb16-4037-afbb-f5fb6080c0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593622c2-d768-4e9b-8423-97971b827b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66233b-0ef5-4a0c-b3ad-9f9cc76d5c40",
   "metadata": {},
   "source": [
    "# Decision Trees & Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347400d-304a-43a8-bd4b-1367188f8667",
   "metadata": {},
   "source": [
    "[Matthew R. Carbone](https://www.bnl.gov/staff/mcarbone) | _Assistant Computational Scientist, Computational Science Initiative, Brookhaven National Laboratory_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263b8e5-b41c-4370-8d1c-146cd7c376a5",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the fundamentals of decision trees and random/decision forests, which are ensembles of decision trees. We will not focus on the technical details. Instead, you will \n",
    "- Learn the general working principles of a decision tree\n",
    "- Get a brief introduction on how they are trained (note that the optimal algorithm for training a decision tree is [NP-hard](https://en.wikipedia.org/wiki/NP-hardness))\n",
    "- See a bit of the math behind how its done (note: not required!)\n",
    "- See how they can work in a real scientific example\n",
    "\n",
    "For some other resources, we recommend looking at\n",
    "- https://developers.google.com/machine-learning/decision-forests/decision-trees (see the entire course, which is excellent)\n",
    "- https://www.mastersindatascience.org/learning/machine-learning-algorithms/decision-tree/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9803ef-60a2-4729-b942-5e80b1234322",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Palmer Penguins dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de660f0-cc5d-40e4-b533-cd363abbe477",
   "metadata": {},
   "source": [
    "To start, we'll be working with the [Palmer Penguins dataset](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)!\n",
    "\n",
    "Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. doi:10.1371/journal.pone.0090081\n",
    "\n",
    "Yes, the penguins have their own [PyPI module](https://github.com/mcnakhaee/palmerpenguins)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa8f6a-9d74-4036-9c20-de3ebcb48726",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337cb24-3e38-4211-8e98-0caf06c7fb6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "penguins = load_penguins()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff61884-d346-4bf0-8670-6354e4ef0bf3",
   "metadata": {},
   "source": [
    "The `penguins` object is just a [Pandas](https://pandas.pydata.org) dataframe. Let's set the objective of our exercise as **predicting the species of penguin** from a subset of the available information. To start, let's simply examine the data and see how many unique species there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893e5f3-ffb9-4e84-b367-7ddb59edd9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80303055-9021-4376-89a8-bfceead283c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penguins[\"species\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf0b10-7d23-487b-87a2-761658189727",
   "metadata": {},
   "source": [
    "Looks like there's 3! There are also some rows in which we do not have any data. We want to drop those using the `dropna()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550c703-c204-4d0c-87d1-cfae1d8d2fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penguins = penguins.dropna()\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5890d29-de82-4653-8556-bc48fb4b0ba4",
   "metadata": {},
   "source": [
    "Let's plot a few histograms of the data, resolved by the penguin species, to get a feeling of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10efa0-f157-43b4-9046-80b6a1620ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "L = len(features)\n",
    "bins = 15\n",
    "fig, axs = plt.subplots(1, L, figsize=(2 * L, 1), sharey=True)\n",
    "\n",
    "for ax, feature in zip(axs, features):\n",
    "\n",
    "    gentoo_feature = penguins[penguins[\"species\"] == \"Gentoo\"][feature]\n",
    "    adelie_feature = penguins[penguins[\"species\"] == \"Adelie\"][feature]\n",
    "    chinstrap_feature = penguins[penguins[\"species\"] == \"Chinstrap\"][feature]\n",
    "\n",
    "    ax.hist(gentoo_feature, label=\"Gentoo\", bins=bins)\n",
    "    ax.hist(adelie_feature, alpha=0.5, label=\"Adelie\", bins=bins)\n",
    "    ax.hist(chinstrap_feature, alpha=0.5, label=\"Chinstrap\", bins=bins)\n",
    "    ax.set_xlabel(feature)\n",
    "\n",
    "axs[0].set_ylabel(\"Counts\")\n",
    "ax.legend(fontsize=6, frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847d137-adfd-41cf-a7e1-6346e1394c66",
   "metadata": {},
   "source": [
    "Clearly, there are some \"decision boundaries\" that can differentiate the different species of penguin! In fact, it appears that all four of the features plotted above exhibit some sort of boundary. `bill_length_mm` can differentiate between Adelie and everything else, and the other three can differentiate between Gentoo and everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc783e3-d1c1-4212-875a-7e47a1a70641",
   "metadata": {},
   "source": [
    "# Entropy and information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30cfbd-c660-4cbe-b816-cd90bcbb71b3",
   "metadata": {},
   "source": [
    "In order to proceed and train a decision tree, it is important to discuss the concept of entropy and information. The information-theoretic [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) is a measure of the amount of \"surprise\" in a variable or dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7141d6b-d581-45b6-886a-7bb87dc7bb7b",
   "metadata": {},
   "source": [
    "## Intuitive explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ac9e7-4033-473a-9349-f65e3cee1395",
   "metadata": {},
   "source": [
    "\"Surprise\" in this context means how likely it is _you_ are to be surprised by the outcome of a random variable or a \"column\" in a dataset. Let's consider the following example cases:\n",
    "- If you have a random variable that can only take on a single value, then the entropy of that random variable is minimized and equal to 0. There is no \"surprise\" since the outcome is deterministic.\n",
    "- If you have a random variable with two possible outcomes each equally likely, then the entropy of that random variable is maximized and equal to 1.\n",
    "- If you have a random variable that can take on many values, each with equal probabilities, then the entropy of that random variable is maximized.\n",
    "\n",
    "There are, of course, many values of entropy in between. Entropy is given by the equation in the \"Mathematical explaination\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0043ba-7511-43ff-86a3-360f0312af3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Mathematical explaination (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0c365-48a1-4a2f-b78c-5ac7df28cce9",
   "metadata": {},
   "source": [
    "The equation for the information-theoretic entropy is\n",
    "\n",
    "$$ S(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x),$$\n",
    "\n",
    "where $p(x)$ is the probability of class $x$ occuring in a dataset or the probability of observing $x$ in some candidate set $\\mathcal{X}.$ Formally, $X$ is a random variable. This is a bit confusing, but just remember that the sum goes over unique possible outcomes. So, in the case of flipping a coin, we can have heads or tails, and thus there are two terms in the sum (which for a fair coin, are equal).\n",
    "\n",
    "What happens if the probability of observing some $x^\\star = 1.$ Necessarily that means all other values for $x$ have $p(x) = 0$ for all $x \\neq x^\\star.$ This means that the entropy $S(X)=0,$ as there is no \"surprise\" factor. The only observed quantity is $x^\\star$ with probability 1.\n",
    "\n",
    "Let's consider now the case where $\\mathcal{X} = \\{x_1, x_2\\}$ and $p(x_1)=p(x_2)=0.5.$ This is the case when entropy is maximized. Let's show this quickly using `scipy.minimize` under the constraint that of course all probabilities sum to 1. Note that minimizing $-S(X)$ is equivalent to maximizing $S(X).$ Note as well that this is general: $S(X)$ is maximized when $p(x_i) = p(x_j)$ for all $x_i, x_j \\in X.$ We show this as a func exercise below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5a9e3-61e8-4783-b496-42874ea67f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, LinearConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d84e1-f1ec-4022-a704-a253d6aab123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -(p * np.log2(p)).sum()\n",
    "\n",
    "def negative_entropy(p):\n",
    "    return -entropy(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad0b0d3-1849-42e8-b6af-50bb7c495437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "set_size = 2\n",
    "epsilon = 1e-4\n",
    "bounds = [(epsilon, 1.0-epsilon) for _ in range(set_size)]\n",
    "coefs = [1.0 for _ in range(set_size)]\n",
    "x0 = np.random.random(set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a876a-e0f7-453e-ae99-ffb995da92b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = minimize(negative_entropy, x0, bounds=bounds, constraints=LinearConstraint(coefs, lb=1.0, ub=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18a046-118c-4f88-8b83-9f0e551565c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a92e01-ac08-4bbe-9e43-1d71e04c7625",
   "metadata": {},
   "source": [
    "### Mathematical proof of entropy maximization (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3f4ce-8adf-44a1-95a9-9aa06e795bcd",
   "metadata": {},
   "source": [
    "[Jensen's Inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) states that\n",
    "\n",
    "$$ f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)].$$\n",
    "\n",
    "Applying this to the formula for entropy, we have\n",
    "\n",
    "$$ S(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) = \\mathbb{E}[-\\log_2 p(x)] \\leq \\log_2 \\mathbb{E}[1/p(x)] = \\log_2 |\\mathcal{X}|.$$\n",
    "\n",
    "The upper bound is achieved when $p(x)$ satisfies a uniform distribution!\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Entropy_(information_theory)#Further_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3784e1-b6b1-4434-9fde-d62337106a16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entropy and information of the Penguins!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309dee1-7275-4643-954f-4b96bdcce8b5",
   "metadata": {},
   "source": [
    "So how does this idea of entropy apply to the Penguins dataset? Let's take the original database $D$ and partition it by the value of the `bill_depth_mm` column:\n",
    "- A subset where `bill_depth_mm <= t`, called $D_1$\n",
    "- A subset where `bill_depth_mm > t`, called $D_2$\n",
    "\n",
    "The variable $t \\in \\mathbb{R}$ is in general any real number, but we can constrain it for the sake of argument by the minimum and maximum values of `bill_depth_mm`. We will try this partitioning for many values of `t`. For each subset, we can calculate the [information gain](https://developers.google.com/machine-learning/decision-forests/binary-classification) of the split. This can be thought of loosely as how well the partitioning discriminates between the target values as a function of $t.$ Particularly, the information gain $I$ is given by\n",
    "\n",
    "$$I(D, D_1, D_2) = S(D) - \\frac{|D_1|}{|D|}S(D_1) - \\frac{|D_2|}{|D|}S(D_2).$$\n",
    "\n",
    "Importantly, in this case, the entropy is evaluated on the _target column_. So while the partitioning of $D_i$ is done using `t`, the entropies are evaluated on the _target_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94e305-7264-4327-be74-4e64be144aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def series_entropy(series):\n",
    "    \"\"\"Evalutes the entropy of a pd.Series [of categorical data].\"\"\"\n",
    "    \n",
    "    unique = np.unique(series)\n",
    "    probs = [(series == uu).mean() for uu in unique]\n",
    "    return entropy(np.array(probs))\n",
    "\n",
    "def get_possible_t_values(series):\n",
    "    \"\"\"Gets the minimum number possible values for t.\"\"\"\n",
    "    \n",
    "    unique = np.unique(series)\n",
    "    diff = np.diff(unique)\n",
    "    return unique[:-1] + diff / 2.0\n",
    "\n",
    "def information_gain_continuous_column_split(\n",
    "    df, target_column_name=\"species\", splitting_column_name=\"bill_depth_mm\"\n",
    "):\n",
    "    \"\"\"Computes the information gain for each of the splits given by t.\"\"\"\n",
    "    \n",
    "    # Compute the original entropy S(X)\n",
    "    series = df[splitting_column_name]\n",
    "    t_array = get_possible_t_values(series)\n",
    "    target_column = df[target_column_name]\n",
    "    original_series_entropy = series_entropy(target_column)\n",
    "    L = len(df.index)\n",
    "    \n",
    "    # Compute the entropy of the new partitions, weighted by their sizes\n",
    "    ig_values = []\n",
    "    for t in t_array:\n",
    "        \n",
    "        df1 = df[df[splitting_column_name] <= t]\n",
    "        L1 = len(df1.index)\n",
    "        df2 = df[df[splitting_column_name] > t]\n",
    "        L2 = len(df2.index)\n",
    "        \n",
    "        d1_entropy = series_entropy(df1[target_column_name])\n",
    "        d2_entropy = series_entropy(df2[target_column_name])\n",
    "        \n",
    "        ig = original_series_entropy - L1 / L * d1_entropy - L2 / L * d2_entropy\n",
    "\n",
    "        ig_values.append(ig)\n",
    "    \n",
    "    return t_array, np.array(ig_values)\n",
    "\n",
    "def print_ig_info(\n",
    "    df=penguins,\n",
    "    splitting_column_name=\"flipper_length_mm\",\n",
    "    target_column_name=\"species\"\n",
    "):\n",
    "\n",
    "    t_array, ig_array = information_gain_continuous_column_split(\n",
    "        df,\n",
    "        target_column_name=target_column_name,\n",
    "        splitting_column_name=splitting_column_name\n",
    "    )\n",
    "    argmax = np.argmax(ig_array)\n",
    "    split_location = t_array[argmax]\n",
    "    max_ig = ig_array[argmax]\n",
    "    print(\n",
    "        f\"The splitting location occurs at {splitting_column_name}=={split_location:.02f}, \"\n",
    "        f\"with IG=={max_ig:.02f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb6314-ce16-4e79-9ff8-ed54b8b59164",
   "metadata": {},
   "source": [
    "Iterating through the numerical columns `splitting_column_name` can show us where each split is made, and what the information gain is for making the split. Keep this in mind for the next section: it looks like the first split _should_ be for the `flipper_length_mm` column at a value of roughly 206."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68f67e-39bf-43d6-97c8-c03794e5a3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]:\n",
    "    print_ig_info(splitting_column_name=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a7a53-43ba-44f3-8b18-c2c032c4ed42",
   "metadata": {},
   "source": [
    "# Training a single decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbc0ea-86de-43e4-8a1a-8ca9eb2f6c4b",
   "metadata": {},
   "source": [
    "The details of training a decision tree efficiently are a bit beyond the scope of this tutorial. However, we can clearly demonstrate the general methodology without loss of value. The general ideas we will present are essentially the same tools used to train decision trees in production-level algorithms. We'll follow along with the tutorial presented [here](https://developers.google.com/machine-learning/decision-forests/practice) (which is also highly recommended!), but we'll use `sklearn` instead of the TensorFlow version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119c1d3-ef58-46ff-9e40-b784501640df",
   "metadata": {},
   "source": [
    "We first actually want to do some data processing. First, we should make sure our targets are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c895de-1588-4944-a63a-193cd0aff66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_column = \"species\"\n",
    "classes = np.unique(penguins[target_column]).tolist()\n",
    "penguins[target_column] = penguins[target_column].map(classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6061834-6cd6-42d9-8eae-5a1c8ef3e4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Unique species are now\", np.unique(penguins[\"species\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493efbb4-194e-4e8c-b307-3ab861344e37",
   "metadata": {},
   "source": [
    "Next, we need to split our dataset into testing and training.\n",
    "\n",
    "**WARNING**: in general, you want to do cross validation and hyperparameter tuning (so you would do a testing, _validation_ and training split). See [Jackson Lee's excellent tutorial](https://indico.bnl.gov/event/18154/), which covers this, for an explanation if you haven't already.\n",
    "\n",
    "For now though, we will not be hyperparameter tuning, just for the sake of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65bfd9-ef70-48b1-8962-31705c20d619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c93e60-07ed-4570-b78d-519e16a1a2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd_train, pd_test = train_test_split(penguins, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e04fc6-29e6-425f-b76d-e6929633e4fb",
   "metadata": {},
   "source": [
    "For features, we'll select the following subset of the columns (keeping the features numerical instead of categorical for simplicity):\n",
    "- `bill_length_mm`\n",
    "- `bill_depth_mm`\n",
    "- `flipper_length_mm`\n",
    "- `body_mass_g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b4857-6633-4c19-91ea-870f754d0080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X_train = pd_train[cols].to_numpy()\n",
    "X_test = pd_test[cols].to_numpy()\n",
    "y_train = pd_train[target_column].to_numpy()\n",
    "y_test = pd_test[target_column].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d457525-fdda-4e80-96b9-63fca06a167a",
   "metadata": {},
   "source": [
    "And now we can initialize, and train, our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a106fb0-21cf-42bb-87ae-2c652377afb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, random_state=123)\n",
    "classifier.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8380d4-a348-4eff-920e-5dc9b86c038c",
   "metadata": {},
   "source": [
    "We can even visualize the tree! More ways to do this [here](https://mljar.com/blog/visualize-decision-tree/). This is a very insightful way to gain some information about how the model is making decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7cfe3-5812-454d-af7c-d6e1653c5cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = plot_tree(\n",
    "    classifier, \n",
    "    feature_names=cols,  \n",
    "    class_names=target_column,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c9294-bbdf-4ef1-9c27-3a7026d25f1f",
   "metadata": {},
   "source": [
    "Note where the first split occurs! It might be slightly different since this splitting is evaluated on the training set and not the entire dataset, but it's close to what we found before, confirming our intuition.\n",
    "\n",
    "This splitting continues _recursively_ until no more splits can be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205973e-b47c-465b-9753-ca2cce9bcb6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec2bd8-3d58-4a05-a89c-bb71d56456c1",
   "metadata": {},
   "source": [
    "A [Random Forest](https://developers.google.com/machine-learning/decision-forests/random-forests) is an ensemble of more than one decision tree (usually some reasonably large number, like 100). Why do we need more than one? What's the point?\n",
    "\n",
    "Consider the principle of [wisdom of the crowd](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd). A single random person's prediction about just about anything is not robust. However, the aggregate opinion of many independent predictions is much more robust. As long as each individual's estimation is not _terrible_ (or random), it is generally the case that the aggregate opinion is more accurate than any individual's.\n",
    "\n",
    "Consider this [fun example](https://www.heart.co.uk/showbiz/tv-movies/who-wants-to-be-a-millionaire-axe-ask-the-audience/):\n",
    "\n",
    "![image](https://imgs.heart.co.uk/images/162692?crop=16_9&width=660&relax=1&signature=bNYg8mltOU1mhoIkDtebHq_wdTs=)\n",
    "\n",
    "If only a single person were asked this question, your confidence is not going to be nearly as high as if the entire audience (hundreds of people) all agree! (Aside: how do 2% of people think Christmas happens on the last day of the month every year??? Imagine if _that_ was the estimator you queried!)\n",
    "\n",
    "Random Forests, and other ensemble models, all operate on this principle. The opinion of a diverse set of (mostly) independent estimators is far more reliable than the opinion of any single estimator.\n",
    "\n",
    "Each estimator in a Random Forest is a decision tree. The way that each tree is trained is very similar to what we've already discussed, though choosing which tree gets which subset of the data, or subset of the features, is an interesting question. See the links above for more details on this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21bfa2-dc85-4826-9411-eb2bd2d33ef1",
   "metadata": {},
   "source": [
    "# Real research example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bd715-4fc9-4700-9636-f76c9c9e4265",
   "metadata": {},
   "source": [
    "We now follow along with the manuscript Torrisi _et al_ to demonstrate how a random forest was used in a somewhat recent, highly cited research work. The data we pull below is available [open access](https://data.matr.io/4/).\n",
    "\n",
    "S. B. Torrisi, M. R. Carbone, B. A. Rohr, J. H. Montoya, Y. Ha, J. Yano, S. K. Suram & L. Hung. [Random forest machine learning models for interpretable X-ray absorption near-edge structure spectrum-property relationships.](https://www.nature.com/articles/s41524-020-00376-6) npj Comput. Mater. 6, 109 (2020).\n",
    "\n",
    "First, we have to get the data. To do this, we use the `requests` module to directly pull the content of the webpage, and then parse that specific format (which despite the extension is not exactly JSON). It is not important to understand the particulars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e874571-bafe-42da-8da0-cf4810f57ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595573ec-f6c4-4e7f-ad28-792e3d1b073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s3.amazonaws.com/publications.matr.io/4/deployment/data/files/spectral_data/Ti_XY.json\"\n",
    "r = requests.get(url)\n",
    "text = r.text.split(\"\\n\")\n",
    "data = [json.loads(xx) for xx in text[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea4d06-f3bb-46e8-8217-b3b44fc18cbd",
   "metadata": {},
   "source": [
    "Get the inputs and outputs from this list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161e1c6-f3ae-4433-a98b-1f4cb73134c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_grid = data[0][\"E\"]\n",
    "spectra = np.array([\n",
    "    dat[\"mu\"] for dat in data\n",
    "    if dat[\"one_hot_coord\"] is not None\n",
    "])\n",
    "coordinations = np.array([\n",
    "    dat[\"coordination\"] for dat in data\n",
    "    if dat[\"one_hot_coord\"] is not None\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b07d9-3e9c-4831-bbfb-9d167d7544f9",
   "metadata": {},
   "source": [
    "Like the Palmer Penguins dataset, we have reduced our classification task to a 3-class classification problem. In this case, it is the coordination number of an X-ray-absorbing atom! If you don't know what this means, don't worry too much about it. We're simply trying to classify whether or not an X-ray absorption spectrum can be used to predict this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3ab7a-949d-4fe0-aa7b-32e9f2bd9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(coordinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce811530-c641-4f13-b0d5-6ab20137d954",
   "metadata": {},
   "source": [
    "Here are what some of the spectra look like. These are our input features, while the classes above are our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399bbda-a374-4be1-b4c8-a98e14bf5f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2, 1))\n",
    "\n",
    "for spec in spectra[::50]:\n",
    "    ax.plot(e_grid, spec, color=\"black\", alpha=0.1)\n",
    "\n",
    "ax.set_ylabel(\"$\\mu(E)$ / a.u.\")\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"$E$ / e.V.\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad7633-7470-491d-b9c6-0f8fd47d2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacef96-cfa6-4c7c-bf04-b25b6f5e5aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spectra, coordinations, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98adc5-6de3-4efd-8606-6c602b02ddf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=6, n_estimators=150, random_state=123)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529b94d-7297-4a6f-bcd2-39d92e9f0584",
   "metadata": {},
   "source": [
    "Note how quickly the random forest trains. Generally speaking, it would be much more expensive in many situations to train e.g. a deep neural network (though in this case there are not many training examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8513d-7047-4eb2-840e-eff0d6f7138b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9cc929-8951-46cb-90f9-1645e609c734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "predictions = rfc.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions, labels=rfc.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rfc.classes_)\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9332add-2973-4617-abc3-257fe543b31b",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31bb037-be4e-4b81-81b7-9c0660fdfc20",
   "metadata": {},
   "source": [
    "Here are some fun topics/questions for you to explore/try to answer. If you have any questions, please feel free to email me!\n",
    "\n",
    "- Prove that the upper bound for maximizing the entropy is satisfied when $p(x) = 1/n,$ where $n$ is the number of unique possible outcomes of a random variable.\n",
    "- Investigate [pruning](https://developers.google.com/machine-learning/decision-forests/overfitting-and-pruning), which is a method for reducing overfitting.\n",
    "- Hard: evaluate the runtime complexity of training a decision tree. Answer [here](https://developers.google.com/machine-learning/decision-forests/binary-classification)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
