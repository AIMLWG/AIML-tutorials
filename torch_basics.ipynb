{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Basics\n",
    "\n",
    "Yihui \"Ray\" Ren \n",
    "yren@bnl.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "\n",
    "* Vectorized Computation\n",
    "    - numpy torch interchangable API\n",
    "    - simple linear regression in numpy and torch\n",
    "* AutoGrad (Automatic Differentiation)\n",
    "    - torch tensor, backward and `grad` \n",
    "    - autograd demo\n",
    "    - `torch.Module` and `forward`.\n",
    "    - re-write linear regression in `torch.Module`\n",
    "* Handling Data \n",
    "    - Stochastic Gradient Descent (SGD)\n",
    "    - `torch.DataSet`\n",
    "    - `torch.DataLoader`\n",
    "    - re-write linear regression\n",
    "* Multi-layer Perceptron\n",
    "    - activation functions\n",
    "* GPU offloading\n",
    "    - parameter and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load modules\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "!date +%D\n",
    "for pkg in [\"np\", \"pd\", \"torch\"]:\n",
    "    print(f\"{pkg:<6} ver: {eval(pkg).__version__}\")\n",
    "\n",
    "# 10/14/21\n",
    "# np     ver: 1.20.3\n",
    "# pd     ver: 1.3.3\n",
    "# torch  ver: 1.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_breaker(foo=None):\n",
    "    return f\"\"\"{\"=\"*30}{foo if foo else \"=\"*20:^20}{\"=\"*30}\"\"\"\n",
    "\n",
    "def plt_linear_fit(x, y, yhat, a, b):\n",
    "    \"\"\"\n",
    "        x, y: input and groud truth\n",
    "        yhat: prediction\n",
    "        a, b: parameters of linear model\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1,2,figsize=(8,4), sharey=True)\n",
    "    ax1, ax2 = axes\n",
    "    ax1.scatter(x, y, facecolor='none', edgecolor='b', alpha=0.1)\n",
    "    ax1.plot(np.linspace(0, 30), a*np.linspace(0,30)+b, 'r')\n",
    "    ax1.set_title(f\"a={a:.3E}, b={b:.3E}\")\n",
    "    ax1.set_xlim([0,30])\n",
    "    ax1.set_ylim([0.985,1.01])\n",
    "    ax1.set_ylabel(\"ground truth y\")\n",
    "    ax1.set_xlabel(\" x \")\n",
    "    ax2.scatter(yhat, y, facecolor='none', edgecolor='b', alpha=0.1)\n",
    "    ax2.set_title(f\"MAE = {np.abs(y-yhat).mean():.5E}\")\n",
    "    ax2.set_ylim([0.985,1.01])\n",
    "    ax2.set_xlim([0.985,1.01])\n",
    "    ax2.set_xlabel(\"yhat\")\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Computation\n",
    "modified from this repo [myazdani/numpy-pytorch-cheatsheet](https://github.com/myazdani/numpy-pytorch-cheatsheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array Creation\n",
    "some_shape = (5,3)\n",
    "some_list = [5,3,2,1]\n",
    "def compare_numpy_torch(np_cb, th_cb, some):\n",
    "    x = np_cb(some)\n",
    "    y = th_cb(some)\n",
    "    x, y = x.shape, y.shape\n",
    "    return x, y\n",
    "\n",
    "for func in [\"empty\", \"ones\", \"zeros\"]:\n",
    "    print(line_breaker(\"comparing \"+func))\n",
    "    npf, thf = eval(\"np.\"+func), eval(\"torch.\"+func)\n",
    "    print(compare_numpy_torch(npf, thf, some_shape))\n",
    "\n",
    "# random tensor\n",
    "print(line_breaker(\"comparing \"+\"rand\"))\n",
    "x = np.random.rand(*some_shape) # np rand does not take a tuple for shape\n",
    "y = torch.rand(some_shape)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "## change random seed, get and set state\n",
    "np.random.seed(5)\n",
    "rng_state = np.random.get_state()\n",
    "np.random.set_state(rng_state)\n",
    "\n",
    "torch.random.manual_seed(5)\n",
    "rng_state = torch.random.get_rng_state()\n",
    "torch.random.set_rng_state(rng_state)\n",
    "\n",
    "# convert between numpy and torch\n",
    "print(line_breaker(\"convert btwn np and torch\"))\n",
    "x = np.random.rand(2,2)\n",
    "y = torch.tensor(x) # convert np to torch\n",
    "z = y.numpy() # convert torch to np\n",
    "assert (x == z).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensor operation\n",
    "x_shape = (3, 3)\n",
    "y_shape = (3, 3)\n",
    "op = \"init\"\n",
    "print(line_breaker(op))\n",
    "npx = np.random.rand(*x_shape)\n",
    "npy = np.random.rand(*y_shape)\n",
    "thx = torch.tensor(npx)\n",
    "thy = torch.tensor(npy)\n",
    "print(\"x:\",npx)\n",
    "print(\"y:\",npy)\n",
    "\n",
    "### add \n",
    "op = \"add\"\n",
    "print(line_breaker(op))\n",
    "npz = npx + npy\n",
    "thz = thx + thy\n",
    "print(\"x+y=z:\", npz)\n",
    "assert (npz == thz.numpy()).all()\n",
    "\n",
    "### mat product\n",
    "op = \"multiply\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npz = npx@npy\n",
    "thz = thx@thy\n",
    "print(\"x@y=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "\n",
    "\n",
    "npz = npx.dot(npy)\n",
    "thz = thx.mm(thy)\n",
    "print(\"x.mm(y)=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "\n",
    "npz = np.matmul(npx,npy)\n",
    "thz = torch.mm(thx, thy)\n",
    "print(\"pkg.mm(x, y)=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "\n",
    "### elementwise mult aka Hadamard product\n",
    "op = \"elementwise multi\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npz = npx*npy\n",
    "thz = thx*thy\n",
    "print(\"x*y=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensor Manipulations\n",
    "def create_test_tensors(x_shape):\n",
    "    npx = np.random.rand(*x_shape)\n",
    "    thx = torch.tensor(npx)\n",
    "    return npx, thx\n",
    "    \n",
    "### transpose\n",
    "op = \"transpose\"\n",
    "print(line_breaker(op))\n",
    "\n",
    "tensor_shape = (1,3)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npxT = npx.T\n",
    "thxT = thx.T\n",
    "\n",
    "tensor_shape = (3,4,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "print(\"before transpose\", npx.shape, thx.shape)\n",
    "# npxT = np.transpose(npx, (1,0,2)) # also works\n",
    "npxT = npx.transpose((1,0,2))\n",
    "# thxT = torch.permute(thx, (1,0,2)) # does not works in torch1.7\n",
    "thxT = thx.permute((1,0,2)) \n",
    "print(\"after transpose \", npxT.shape, thxT.shape)\n",
    "\n",
    "### flatten and reshape \n",
    "op = \"flatten\"\n",
    "print(line_breaker(op))\n",
    "tensor_shape = (3,4,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npflat1 = npx.reshape(-1)\n",
    "npflat2 = npx.flatten()\n",
    "thflat1 = thx.reshape(-1)\n",
    "thflat2 = thx.flatten()\n",
    "thflat3 = thx.view(-1)\n",
    "thflat4 = torch.flatten(thx)\n",
    "for x in [npflat1, npflat2, thflat1, thflat2, thflat3, thflat4]:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Squeeze and Unsqueeze (adding and removing dummy dimensions)\n",
    "op = \"squeeze\"\n",
    "print(line_breaker(op))\n",
    "tensor_shape = (3,1,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "print(\"before squeeze\", npx.shape, thx.shape)\n",
    "npxs = npx.squeeze() \n",
    "thxs = thx.squeeze() \n",
    "print(\"after squeeze \", npxs.shape, thxs.shape)\n",
    "op = \"unsqueeze\"\n",
    "print(line_breaker(op))\n",
    "npxus = np.expand_dims(npxs,1)\n",
    "thxus = thxs.unsqueeze(1)\n",
    "print(\"after unsqueeze at dim 1:\", npxus.shape, thxus.shape)\n",
    "\n",
    "### Concat \n",
    "op = \"concatenate\"\n",
    "print(line_breaker(op))\n",
    "tensor_shape = (3,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npy, thy = create_test_tensors(tensor_shape)\n",
    "print(\"before concat\", npx.shape, npy.shape)\n",
    "npz0 = np.concatenate((npx, npy), axis=0)\n",
    "thz0 = torch.cat((thx, thy), axis=0)\n",
    "assert npz0.shape == thz0.shape\n",
    "print(\"after concat along dim 0:\", npz0.shape)\n",
    "npz1 = np.concatenate((npx, npy), axis=1)\n",
    "thz1 = torch.cat((thx, thy), axis=1)\n",
    "assert npz1.shape == thz1.shape\n",
    "print(\"after concat along dim 1:\", npz1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using Vectorized Computation\n",
    "\n",
    "### Wine Quality Dataset\n",
    "### Linear regression using a formula\n",
    "### Homework: convert numpy implementation to torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get data for linear regression. \n",
    "wine_quality_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "df = pd.read_csv(wine_quality_url, delimiter=\";\")\n",
    "w = df.corr()\n",
    "sns.heatmap(w)\n",
    "up_tri = np.triu(np.abs(w.to_numpy()),k=1)\n",
    "max_idx = np.argmax(up_tri)\n",
    "col_sz = len(df.columns)\n",
    "col1, col2 = df.columns[max_idx//col_sz], df.columns[max_idx%col_sz]\n",
    "print(max_idx,up_tri.flatten()[max_idx], col1, col2)\n",
    "\n",
    "print(\"found {col1} and {col2} for linear regression\") # most correlated two features\n",
    "plt.figure()\n",
    "sns.scatterplot(x=df[col1], y=df[col2])\n",
    "plt.title(f\"{col1} and {col2} \\n corr.coef. = {np.corrcoef(df[col1], df[col2])[0,1]:.4f}\")\n",
    "\n",
    "wine_x, wine_y = df[col1].to_numpy(), df[col2].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression \n",
    "Find a and b such that:\n",
    "$\\sum (y-y')^2$\n",
    "is minimized, where\n",
    "$y' = ax+b$\n",
    "\n",
    "with solution \n",
    "\n",
    "$a = \\sum(x - \\bar{x})(y - \\bar{y}) / \\sum (x - \\bar{x})^2$\n",
    "\n",
    "$b = \\bar{y} - a\\bar{x}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n10 -r10\n",
    "## here is a numpy implementation\n",
    "xbar = np.mean(wine_x)\n",
    "ybar = np.mean(wine_y)\n",
    "a = (wine_x - xbar)@(wine_y-ybar).T / np.power(wine_x-xbar, 2).sum()\n",
    "b = ybar - a * xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_linear_fit(wine_x, wine_y, a*wine_x+b, a, b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: re-write above code in torch\n",
    "here is the torch doc https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thx = torch.tensor(wine_x)\n",
    "thy = torch.tensor(wine_y)\n",
    "##                            ##\n",
    "##                            ##\n",
    "##  Intentionally Left Blank  ## \n",
    "##                            ##\n",
    "##                            ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n10 -r10\n",
    "## Solution\n",
    "thx = torch.tensor(wine_x)\n",
    "thy = torch.tensor(wine_y)\n",
    "xbar = torch.mean(thx)\n",
    "ybar = torch.mean(thy)\n",
    "a = (thx - xbar)@(thy-ybar).T / torch.pow(thx-xbar, 2).sum()\n",
    "b = ybar - a * xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_linear_fit(wine_x, wine_y, (a*thx+b).numpy(), a.numpy(), b.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad\n",
    "In this section, you will learn automated differentiation.\n",
    "1. Torch tensor has built-in grad\n",
    "2. Wrap tensors and operations into `nn.Module` so they are chained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## y = a*x, dy/dx = ? \n",
    "x = torch.tensor(0.1, requires_grad=True)\n",
    "a = torch.tensor(3)\n",
    "y = a*x # y=ax, dy/dx = a\n",
    "y.backward()\n",
    "print(\"variable x\", x, \"has grad of\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here is another example\n",
    "## y = exp(a*x), dy/dx = ? \n",
    "## see if this is what you expected.\n",
    "x = torch.tensor(0.3, requires_grad=True)\n",
    "a = torch.tensor(3)\n",
    "y = torch.exp(a*x) \n",
    "# do it by hand using chain rule: \n",
    "# y = exp(ax), dy/dx = exp(ax) d(ax)/dx = a * exp(ax)\n",
    "print(\"do it by hand:\\n\", \"y = exp(ax), dy/dx = exp(ax) d(ax)/dx = a * exp(ax)\")\n",
    "y.backward()\n",
    "print(\"variable x\", x, \"has grad of\", x.grad, \"which should be the same as\", a*torch.exp(a*x).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: pick a f(x) you like, and autograd it!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##                            ##\n",
    "##                            ##\n",
    "##  Intentionally Left Blank  ## \n",
    "##                            ##\n",
    "##                            ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider this function: $ y = ( \\sin x + 1 )^x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution:  \n",
    "## for example y = (sin(x)+1)^x\n",
    "print(\"for example y = (sin(x)+1)^x\")\n",
    "x = torch.tensor(0.2, requires_grad=True)\n",
    "y = (torch.sin(x)+1).pow(x)\n",
    "y.backward()\n",
    "ans = torch.exp(x*torch.log(torch.sin(x)+1))*((torch.log(torch.sin(x)+1))+x*(torch.sin(x)+1).pow(-1)*torch.cos(x))\n",
    "print(x, x.grad, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Torch Module \n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "# torch.module: Packing parameters and functions together\n",
    "# two APIs:\n",
    "# __init__() and forward()\n",
    "\n",
    "class Func(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(0.2, requires_grad=True) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (self.x.sin()+1).pow(self.x)\n",
    "    \n",
    "## create a module \n",
    "func = Func()\n",
    "y = func(None)\n",
    "y.backward()\n",
    "print(func.x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error: if requires_grad=False\n",
    "print(\"Warning: this will produce error\")\n",
    "class FuncError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(0.2) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (self.x.sin()+1).pow(self.x)\n",
    "    \n",
    "## create a module \n",
    "func = FuncError()\n",
    "y = func(None)\n",
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as err:\n",
    "    print(err)\n",
    "    \n",
    "print(\"func.x.grad is\", func.x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better to use torch.nn.Parameter\n",
    "## and rename x as w (as weights) as a convention.\n",
    "\n",
    "class FuncPara(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        w = torch.tensor(0.2)\n",
    "        self.w = nn.Parameter(w)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (self.w.sin()+1).pow(self.w)\n",
    "    \n",
    "## create a module \n",
    "func = FuncPara()\n",
    "y = func(None)\n",
    "y.backward()\n",
    "print(func.w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [Optional]\n",
    "## The benefits of using Parameter are two folds: \n",
    "#  * registered to module parameters. \n",
    "#  * moves with modules to device.\n",
    "\n",
    "print(line_breaker())\n",
    "print(\"nn.Parameters are registered to nn.Module\")\n",
    "func = FuncPara()\n",
    "for p in func.parameters():\n",
    "    print(p)\n",
    "    \n",
    "print(line_breaker())\n",
    "print(\"torch.tensor did not\")\n",
    "func = Func()\n",
    "for p in func.parameters():\n",
    "    print(p)\n",
    "print(\"got nothing\")\n",
    "print(line_breaker())\n",
    "\n",
    "print(line_breaker(\"move btwn cpu&gpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"registered parameter moves with Module\")\n",
    "    func_p = FuncPara()\n",
    "    print(func_p.w.device)\n",
    "    # also works on GPU\n",
    "    func_p = func_p.cuda()\n",
    "    print(func_p.w.device)\n",
    "    \n",
    "    print(\"unregistered tensor does not\")\n",
    "    # if Func tensor\n",
    "    func_t = Func()\n",
    "    print(func_t.x.device)\n",
    "    # also works on GPU\n",
    "    func_t = func_t.cuda()\n",
    "    print(func_t.x.device)\n",
    "    \n",
    "    ## print out:\n",
    "    #  cpu\n",
    "    #  cuda:0\n",
    "    #  cpu\n",
    "    #  cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules can be composed together or called in sequence, auto-grad will work in both cases.\n",
    "\n",
    "Let's still use this function: $ y = ( \\sin x + 1 )^x $, and break it into two simpler ones $\\sin(x)$ and $u^x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modules can be composed and auto grad works \n",
    "## let's break the previous function into two parts, \n",
    "class SinFunc(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "    def forward(self):\n",
    "        return torch.sin(self.x)\n",
    "    \n",
    "class PowFunc(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "    def forward(self, input):\n",
    "        return torch.pow(input, self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(0.2, requires_grad=True)\n",
    "x = nn.Parameter(x)\n",
    "\n",
    "f = SinFunc(x)\n",
    "g = PowFunc(x)\n",
    "y = g(f()+1)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineFunc(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.f = SinFunc(x)\n",
    "        self.g = PowFunc(x)\n",
    "    def forward(self):\n",
    "        return self.g(self.f()+1)\n",
    "h = CombineFunc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = h()\n",
    "y.backward()\n",
    "x.grad\n",
    "# what's the value of x.grad? why is it? what if you run several times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's get back to the linear regression case.\n",
    "## we can solve it another way using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.random.manual_seed(7) \n",
    "        # some random guess of initial values\n",
    "        self.a = nn.Parameter(torch.rand(1), requires_grad=True) \n",
    "        self.b = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.a + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thx = torch.tensor(wine_x)\n",
    "thy = torch.tensor(wine_y)\n",
    "lin = LinearModel()\n",
    "yhat = lin(thx)\n",
    "V = lambda x: x.cpu().detach().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.003 # learning rate\n",
    "for ep in range(10**4):\n",
    "    yhat = lin(thx)\n",
    "    loss = ((thy-yhat)*(thy-yhat)).mean()\n",
    "    loss.backward()\n",
    "    for p in lin.parameters():\n",
    "        # Q: what is p? \n",
    "        p.data -= lr * p.grad\n",
    "        p.grad.zero_() \n",
    "        # Q: recall why we need to set grad to zero?\n",
    "    \n",
    "    if ep%10**3==0:\n",
    "        clear_output()\n",
    "        plt.close(fig)\n",
    "        fig = plt_linear_fit(wine_x, wine_y, V(lin(thx)), V(lin.a), V(lin.b))\n",
    "        display(fig)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Built-in Modules, DataLoader and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class WineData(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.astype(float)\n",
    "        self.y = y.astype(float)\n",
    "        self.x = np.expand_dims(self.x, axis=1).astype(float)\n",
    "        self.y = np.expand_dims(self.y, axis=1).astype(float)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "# examples:\n",
    "dataset = WineData(wine_x, wine_y)\n",
    "print(\"data sz = \", len(dataset), \"\\nget 3rd item\",dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineData(wine_x, wine_y)\n",
    "dataloader = DataLoader(dataset, batch_size=2048, drop_last=True, shuffle=True)\n",
    "model = LinearModel() \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "print(x.shape, y.shape)\n",
    "yhat = model(x)\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(10**3):\n",
    "    for x, y in dataloader:\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    if ep%10**2==0:\n",
    "        clear_output()\n",
    "        plt.close(fig)\n",
    "        yhat = model(torch.tensor(wine_x))\n",
    "        fig = plt_linear_fit(wine_x, wine_y, V(yhat), V(model.a), V(model.b))\n",
    "        display(fig)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: try to replace your LinearModel, with the built-in one:\n",
    "# model = nn.Linear(1,1,bias=True) \n",
    "# and inspect  model.weight and model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "for ep in range(10**3):\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    if ep%10**2==0:\n",
    "        clear_output()\n",
    "        plt.close(fig)\n",
    "        yhat = model(torch.tensor(wine_x).cuda())\n",
    "        fig = plt_linear_fit(wine_x, wine_y, V(yhat), V(model.a), V(model.b))\n",
    "        display(fig)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
