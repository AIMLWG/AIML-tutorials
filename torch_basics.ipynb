{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Basics\n",
    "\n",
    "Yihui \"Ray\" Ren \n",
    "yren@bnl.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "\n",
    "* Vectorized Computation\n",
    "    - numpy torch interchangable API\n",
    "    - simple linear regression in numpy and torch\n",
    "* AutoGrad (Automatic Differentiation)\n",
    "    - torch tensor, backward and `grad` \n",
    "    - autograd demo\n",
    "    - `torch.Module` and `forward`.\n",
    "    - re-write linear regression in `torch.Module`\n",
    "* Handling Data \n",
    "    - Stochastic Gradient Descent (SGD)\n",
    "    - `torch.DataSet`\n",
    "    - `torch.DataLoader`\n",
    "    - re-write linear regression\n",
    "* Multi-layer Perceptron\n",
    "    - activation functions\n",
    "* GPU offloading\n",
    "    - parameter and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for pkg in [\"np\", \"pd\", \"torch\"]:\n",
    "    print(f\"{pkg:<6} ver: {eval(pkg).__version__}\")\n",
    "\n",
    "## last tested on \n",
    "#  np     ver: 1.18.5\n",
    "#  pd     ver: 1.1.3\n",
    "#  torch  ver: 1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Computation\n",
    "modified from this repo [myazdani/numpy-pytorch-cheatsheet](https://github.com/myazdani/numpy-pytorch-cheatsheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Array Creation\n",
    "some_shape = (5,3)\n",
    "some_list = [5,3,2,1]\n",
    "def compare_numpy_torch(np_cb, th_cb, some):\n",
    "    x = np_cb(some)\n",
    "    y = th_cb(some)\n",
    "    x, y = x.shape, y.shape\n",
    "    return x, y\n",
    "\n",
    "for func in [\"empty\", \"ones\", \"zeros\"]:\n",
    "    print(\"compare\", func)\n",
    "    npf, thf = eval(\"np.\"+func), eval(\"torch.\"+func)\n",
    "    print(compare_numpy_torch(npf, thf, some_shape))\n",
    "\n",
    "# random tensor\n",
    "print(\"compare\", \"rand\")\n",
    "x = np.random.rand(*some_shape) # np rand does not take a tuple for shape\n",
    "y = torch.rand(some_shape)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# change random seed, get and set state\n",
    "np.random.seed(5)\n",
    "rng_state = np.random.get_state()\n",
    "np.random.set_state(rng_state)\n",
    "\n",
    "torch.random.manual_seed(5)\n",
    "rng_state = torch.random.get_rng_state()\n",
    "torch.random.set_rng_state(rng_state)\n",
    "\n",
    "# convert from numpy and torch\n",
    "x = np.random.rand(2,2)\n",
    "y = torch.tensor(x) # convert np to torch\n",
    "z = y.numpy() # convert torch to np\n",
    "assert (x == z).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensor operation\n",
    "x_shape = (3, 3)\n",
    "y_shape = (3, 3)\n",
    "op = \"init\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npx = np.random.rand(*x_shape)\n",
    "npy = np.random.rand(*y_shape)\n",
    "thx = torch.tensor(npx)\n",
    "thy = torch.tensor(npy)\n",
    "print(\"x:\",npx)\n",
    "print(\"y:\",npy)\n",
    "\n",
    "### add \n",
    "op = \"add\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npz = npx + npy\n",
    "thz = thx + thy\n",
    "print(\"x+y=z:\", npz)\n",
    "assert (npz == thz.numpy()).all()\n",
    "\n",
    "### mat product\n",
    "op = \"multiply\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npz = npx@npy\n",
    "thz = thx@thy\n",
    "print(\"x@y=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "\n",
    "\n",
    "npz = npx.dot(npy)\n",
    "thz = thx.mm(thy)\n",
    "print(\"x.mm(y)=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "\n",
    "npz = np.matmul(npx,npy)\n",
    "thz = torch.mm(thx, thy)\n",
    "print(\"pkg.mm(x, y)=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()\n",
    "\n",
    "### elementwise mult aka Hadamard product\n",
    "op = \"elementwise multi\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npz = npx*npy\n",
    "thz = thx*thy\n",
    "print(\"x*y=z:\", npz)\n",
    "assert np.isclose(npz, thz.numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensor Manipulations\n",
    "def create_test_tensors(x_shape):\n",
    "    npx = np.random.rand(*x_shape)\n",
    "    thx = torch.tensor(npx)\n",
    "    return npx, thx\n",
    "    \n",
    "### transpose\n",
    "op = \"transpose\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "\n",
    "tensor_shape = (1,3)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npxT = npx.T\n",
    "thxT = thx.T\n",
    "\n",
    "tensor_shape = (3,4,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "print(\"before transpose\", npx.shape, thx.shape)\n",
    "# npxT = np.transpose(npx, (1,0,2)) # also works\n",
    "npxT = npx.transpose((1,0,2))\n",
    "# thxT = torch.permute(thx, (1,0,2)) # does not works in torch1.7\n",
    "thxT = thx.permute((1,0,2)) \n",
    "print(\"after transpose \", npxT.shape, thxT.shape)\n",
    "\n",
    "### flatten and reshape \n",
    "op = \"flatten\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "tensor_shape = (3,4,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npflat1 = npx.reshape(-1)\n",
    "npflat2 = npx.flatten()\n",
    "thflat1 = thx.reshape(-1)\n",
    "thflat2 = thx.flatten()\n",
    "thflat3 = thx.view(-1)\n",
    "thflat4 = torch.flatten(thx)\n",
    "for x in [npflat1, npflat2, thflat1, thflat2, thflat3, thflat4]:\n",
    "    print(x.shape)\n",
    "\n",
    "### Squeeze and Unsqueeze (adding and removing dummy dimensions)\n",
    "op = \"squeeze\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "tensor_shape = (3,1,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "print(\"before squeeze\", npx.shape, thx.shape)\n",
    "npxs = npx.squeeze() \n",
    "thxs = thx.squeeze() \n",
    "print(\"after squeeze \", npxs.shape, thxs.shape)\n",
    "op = \"unsqueeze\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "npxus = np.expand_dims(npxs,1)\n",
    "thxus = thxs.unsqueeze(1)\n",
    "print(\"after unsqueeze at dim 1:\", npxus.shape, thxus.shape)\n",
    "\n",
    "### Concat \n",
    "op = \"concatenate\"\n",
    "print(f\"\"\"{\"=\"*30}{op:^20}{\"=\"*30}\"\"\")\n",
    "tensor_shape = (3,5)\n",
    "npx, thx = create_test_tensors(tensor_shape)\n",
    "npy, thy = create_test_tensors(tensor_shape)\n",
    "print(\"before concat\", npx.shape, npy.shape)\n",
    "npz0 = np.concatenate((npx, npy), axis=0)\n",
    "thz0 = torch.cat((thx, thy), axis=0)\n",
    "assert npz0.shape == thz0.shape\n",
    "print(\"after concat along dim 0:\", npz0.shape)\n",
    "npz1 = np.concatenate((npx, npy), axis=1)\n",
    "thz1 = torch.cat((thx, thy), axis=1)\n",
    "assert npz1.shape == thz1.shape\n",
    "print(\"after concat along dim 1:\", npz1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get data for linear regression. \n",
    "wine_quality_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "df = pd.read_csv(wine_quality_url, delimiter=\";\")\n",
    "w = df.corr()\n",
    "sns.heatmap(w)\n",
    "up_tri = np.triu(np.abs(w.to_numpy()),k=1)\n",
    "max_idx = np.argmax(up_tri)\n",
    "col_sz = len(df.columns)\n",
    "col1, col2 = df.columns[max_idx//col_sz], df.columns[max_idx%col_sz]\n",
    "print(max_idx,up_tri.flatten()[max_idx], col1, col2)\n",
    "\n",
    "print(\"found {col1} and {col2} for linear regression\")\n",
    "plt.figure()\n",
    "sns.scatterplot(x=df[col1], y=df[col2])\n",
    "plt.title(f\"{col1} and {col2} \\n corr.coef. = {np.corrcoef(df[col1], df[col2])[0,1]:.4f}\")\n",
    "\n",
    "wine_x, wine_y = df[col1].to_numpy(), df[col2].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple linear regression \n",
    "find a and b such that  \n",
    "$\\sum (y-y')^2$\n",
    "is minimized, where\n",
    "$y' = ax+b$\n",
    "\n",
    "with solution \n",
    "\n",
    "$a = \\sum(x - \\bar{x})(y - \\bar{y}) / \\sum (x - \\bar{x})^2$\n",
    "\n",
    "$b = \\bar{y} - a\\bar{x}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbar = np.mean(wine_x)\n",
    "ybar = np.mean(wine_y)\n",
    "a = (wine_x - xbar)@(wine_y-ybar).T / np.power(wine_x-xbar, 2).sum()\n",
    "b = ybar - a * xbar\n",
    "plt.scatter(wine_x, wine_y, facecolor='none', edgecolor='b', alpha=0.1)\n",
    "plt.plot(np.linspace(0, 30), a*np.linspace(0,30)+b, 'r')\n",
    "plt.xlim([0,30])\n",
    "plt.ylim([0.985,1.01])\n",
    "plt.title(f\"mse = {np.power(wine_y - (a*wine_x+b), 2).mean():.4E}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: re-write in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "thx = torch.tensor(wine_x)\n",
    "thy = torch.tensor(wine_y)\n",
    "xbar = torch.mean(thx)\n",
    "ybar = torch.mean(thy)\n",
    "a = (thx - xbar)@(thy-ybar).T / torch.pow(thx-xbar, 2).sum()\n",
    "b = ybar - a * xbar\n",
    "plt.scatter(wine_x, wine_y, facecolor='none', edgecolor='b', alpha=0.1)\n",
    "plt.plot(np.linspace(0, 30), a*np.linspace(0,30)+b, 'r')\n",
    "plt.xlim([0,30])\n",
    "plt.ylim([0.985,1.01])\n",
    "plt.title(f\"mse = {torch.pow(thy - (a*thx+b), 2).mean():.4E}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## y = a*x, dy/dx = ? \n",
    "x = torch.tensor(0.1, requires_grad=True)\n",
    "a = torch.tensor(3)\n",
    "y = a*x # y=ax, dy/dx = a\n",
    "y.backward()\n",
    "print(x, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## y = exp(a*x), dy/dx = ? \n",
    "x = torch.tensor(0.3, requires_grad=True)\n",
    "a = torch.tensor(3)\n",
    "y = torch.exp(a*x) # y = exp(ax), dy/dx = exp(ax) d(ax)/dx = exp(ax) * a\n",
    "y.backward()\n",
    "print(x, x.grad, a*torch.exp(a*x).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: pick a f(x) you like, and autograd it!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution:  for example y = (sin(x)+1)^x\n",
    "x = torch.tensor(0.2, requires_grad=True)\n",
    "y = (torch.sin(x)+1).pow(x)\n",
    "y.backward()\n",
    "ans = torch.exp(x*torch.log(torch.sin(x)+1))*((torch.log(torch.sin(x)+1))+x*(torch.sin(x)+1).pow(-1)*torch.cos(x))\n",
    "print(x, x.grad, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Torch Module \n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "# torch.module: Packing parameters and functions together\n",
    "# two APIs, __init__() and forward()\n",
    "\n",
    "class Func(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(0.2, requires_grad=True) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (self.x.sin()+1).pow(self.x)\n",
    "    \n",
    "## create a module \n",
    "func = Func()\n",
    "y = func(None)\n",
    "y.backward()\n",
    "print(func.x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error: if requires_grad=False\n",
    "print(\"Warning: will produce error\")\n",
    "class FuncError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(0.2) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (self.x.sin()+1).pow(self.x)\n",
    "    \n",
    "## create a module \n",
    "func = FuncError()\n",
    "y = func(None)\n",
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as err:\n",
    "    print(err)\n",
    "    \n",
    "print(func.x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better to use torch.nn.Parameter\n",
    "## and name differently\n",
    "\n",
    "class FuncPara(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        w = torch.tensor(0.2)\n",
    "        self.w = nn.Parameter(w)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (self.w.sin()+1).pow(self.w)\n",
    "    \n",
    "## create a module \n",
    "func = FuncPara()\n",
    "y = func(None)\n",
    "y.backward()\n",
    "print(func.w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The benefits are two folds: \n",
    "#  * registered to module parameters. \n",
    "#  * moves with modules to device.\n",
    "\n",
    "print(\"iterating parameters\")\n",
    "for p in func.parameters():\n",
    "    print(p)\n",
    "    \n",
    "print(\"iterating parameters of the one using tensor\")\n",
    "func = Func()\n",
    "for p in func.parameters():\n",
    "    print(p)\n",
    "print(\"got nothing\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # if FuncPara\n",
    "    func_p = FuncPara()\n",
    "    print(func_p.w.device)\n",
    "    # also works on GPU\n",
    "    func_p = func_p.cuda()\n",
    "    print(func_p.w.device)\n",
    "    \n",
    "    # if Func tensor\n",
    "    func_t = Func()\n",
    "    print(func_t.x.device)\n",
    "    # also works on GPU\n",
    "    func_t = func_t.cuda()\n",
    "    print(func_t.x.device)\n",
    "    \n",
    "    # print out:\n",
    "    # cpu\n",
    "    # cuda:0\n",
    "    # cpu\n",
    "    # cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
